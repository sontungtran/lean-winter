{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "from os.path import expanduser\n",
    "# from functools import lru_cache\n",
    "\n",
    "# @lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "        self.special_tokens = []\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.re_pattern = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        self.pat = re.compile(self.re_pattern)\n",
    "        \n",
    "    def add_special_token(self, s):\n",
    "        if s in self.special_tokens:\n",
    "            print(f\"Token {s} already added\")\n",
    "        self.special_tokens.append(s)\n",
    "        self.encoder[s] = len(self.encoder)+1\n",
    "        self.decoder[len(self.encoder)+1] = s\n",
    "        s = s.replace(\"|\", r\"\\|\")\n",
    "        self.re_pattern = \" \" + s + \"|\" + self.re_pattern\n",
    "        self.pat = re.compile(self.re_pattern)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        # Also don't split on special tokens\n",
    "#         print(token, token in self.special_tokens)\n",
    "        if token in self.special_tokens:\n",
    "            return token\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder():\n",
    "    home = expanduser(\"~\")\n",
    "    with open(os.path.join(home, 'data/GPT2/encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(home, 'data/GPT2/vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    enc = Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )\n",
    "    enc.add_special_token(\"<|endoftext|>\")\n",
    "    enc.add_special_token(\"<|GOAL|>\")\n",
    "    enc.add_special_token(\"<|PROOFSTEP|>\")\n",
    "    print(enc.special_tokens)\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200298\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>goal_pp</th>\n",
       "      <th>decl_name</th>\n",
       "      <th>open_namespaces</th>\n",
       "      <th>filename</th>\n",
       "      <th>line</th>\n",
       "      <th>column</th>\n",
       "      <th>proof_key</th>\n",
       "      <th>human_tactic_code</th>\n",
       "      <th>tactic_class</th>\n",
       "      <th>cleaned_goal</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>α : Type u,\\n_inst_1 : inhabited α,\\nb : buffe...</td>\n",
       "      <td>buffer.read_eq_read'</td>\n",
       "      <td>buffer</td>\n",
       "      <td>lean/library/data/buffer.lean</td>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>lean/library/data/buffer.lean:49:1</td>\n",
       "      <td>cases b; unfold read read'; simp [array.read_e...</td>\n",
       "      <td>semicolon</td>\n",
       "      <td>α : Type u,\\t_inst_1 : inhabited α,\\tb : buffe...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>α : Type u,\\n_inst_1 : inhabited α,\\nb : buffe...</td>\n",
       "      <td>buffer.read_eq_read'</td>\n",
       "      <td>buffer</td>\n",
       "      <td>lean/library/data/buffer.lean</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "      <td>lean/library/data/buffer.lean:49:1</td>\n",
       "      <td>cases b; unfold read read'</td>\n",
       "      <td>semicolon</td>\n",
       "      <td>α : Type u,\\t_inst_1 : inhabited α,\\tb : buffe...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>α : Type u,\\n_inst_1 : inhabited α,\\ni b_fst :...</td>\n",
       "      <td>buffer.read_eq_read'</td>\n",
       "      <td>buffer</td>\n",
       "      <td>lean/library/data/buffer.lean</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>lean/library/data/buffer.lean:49:1</td>\n",
       "      <td>unfold read read'</td>\n",
       "      <td>named</td>\n",
       "      <td>α : Type u,\\t_inst_1 : inhabited α,\\ti b_fst :...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>α : Type u,\\n_inst_1 : inhabited α,\\ni b_fst :...</td>\n",
       "      <td>buffer.read_eq_read'</td>\n",
       "      <td>buffer</td>\n",
       "      <td>lean/library/data/buffer.lean</td>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>lean/library/data/buffer.lean:49:1</td>\n",
       "      <td>simp [array.read_eq_read']</td>\n",
       "      <td>named</td>\n",
       "      <td>α : Type u,\\t_inst_1 : inhabited α,\\ti b_fst :...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>α : Type u,\\n_inst_1 : inhabited α,\\nb : buffe...</td>\n",
       "      <td>buffer.read_eq_read'</td>\n",
       "      <td>buffer</td>\n",
       "      <td>lean/library/data/buffer.lean</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>lean/library/data/buffer.lean:49:1</td>\n",
       "      <td>cases b</td>\n",
       "      <td>named</td>\n",
       "      <td>α : Type u,\\t_inst_1 : inhabited α,\\tb : buffe...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            goal_pp  \\\n",
       "0           0  α : Type u,\\n_inst_1 : inhabited α,\\nb : buffe...   \n",
       "1           1  α : Type u,\\n_inst_1 : inhabited α,\\nb : buffe...   \n",
       "2           2  α : Type u,\\n_inst_1 : inhabited α,\\ni b_fst :...   \n",
       "3           3  α : Type u,\\n_inst_1 : inhabited α,\\ni b_fst :...   \n",
       "4           4  α : Type u,\\n_inst_1 : inhabited α,\\nb : buffe...   \n",
       "\n",
       "              decl_name open_namespaces                       filename  line  \\\n",
       "0  buffer.read_eq_read'          buffer  lean/library/data/buffer.lean    49   \n",
       "1  buffer.read_eq_read'          buffer  lean/library/data/buffer.lean    49   \n",
       "2  buffer.read_eq_read'          buffer  lean/library/data/buffer.lean    49   \n",
       "3  buffer.read_eq_read'          buffer  lean/library/data/buffer.lean    49   \n",
       "4  buffer.read_eq_read'          buffer  lean/library/data/buffer.lean    49   \n",
       "\n",
       "   column                           proof_key  \\\n",
       "0      30  lean/library/data/buffer.lean:49:1   \n",
       "1      11  lean/library/data/buffer.lean:49:1   \n",
       "2      13  lean/library/data/buffer.lean:49:1   \n",
       "3      32  lean/library/data/buffer.lean:49:1   \n",
       "4       4  lean/library/data/buffer.lean:49:1   \n",
       "\n",
       "                                   human_tactic_code tactic_class  \\\n",
       "0  cases b; unfold read read'; simp [array.read_e...    semicolon   \n",
       "1                         cases b; unfold read read'    semicolon   \n",
       "2                                  unfold read read'        named   \n",
       "3                         simp [array.read_eq_read']        named   \n",
       "4                                            cases b        named   \n",
       "\n",
       "                                        cleaned_goal split  \n",
       "0  α : Type u,\\t_inst_1 : inhabited α,\\tb : buffe...  test  \n",
       "1  α : Type u,\\t_inst_1 : inhabited α,\\tb : buffe...  test  \n",
       "2  α : Type u,\\t_inst_1 : inhabited α,\\ti b_fst :...  test  \n",
       "3  α : Type u,\\t_inst_1 : inhabited α,\\ti b_fst :...  test  \n",
       "4  α : Type u,\\t_inst_1 : inhabited α,\\tb : buffe...  test  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"~/data/cleaned_training_data/data_and_metadata.csv\")\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 22 indices\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "prev_length = len(df)\n",
    "df = df[df.goal_pp.apply(lambda x: not pd.isna(x))]\n",
    "df = df.reset_index()\n",
    "print(f\"Dropped {prev_length - len(df)} indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAgTUlEQVR4nO3dfXBd9X3n8fdHkiWwDRiwePJDLIKhdTKp46g2aQtLZpvGZtK4yU4TQ2ZMaFLXW+hsm2Y2kEw3NG13E1rKLBuCSzaeQDfGSYc8uKlTQtINtNkYLDeGYMAg28QW2Fi2sTGWsSzpu3+cn8zxPVe6R7KwZOvzmrmje39P5/fTle5H5+FeKSIwMzPLqxvtCZiZ2djjcDAzswKHg5mZFTgczMyswOFgZmYFDgczMytwONiYJen7km4YRr+QdNmbMaca271GUscJ9L9N0v9J92dKek1S/QjNbYWkPxuJeVYZ+ypJm0dqPBsbHA42LOmFq//WJ+lw7vFHhzHesRfGfhGxKCLuG7lZj6w3M4QiYntETI6I3hpz+Jikfysx3vKI+IuRmFvluiPiXyPiipEY28aOhtGegJ2aImJy/31JLwCfiIgfjt6MbCCS6muFjFkl7znYiJI0X9JPJe2XtFPSlyQ15urfJulhSfskvSzpM5IWAp8BPpL2PJ5IbX8s6RO5vr8v6RlJByU9LWleifk0SfobSdvT9lZIOjPVXSOpQ9KfStqd5ntjru/5kv5R0quS1kv6y/6/0iU9mpo9keb8kVy/quNVmVuLpEfSeh4GpubqZqW/0BvS449J2prabpP0UUm/DKwA3p3msD+1/ZqkeyStlXQIeE8q+8uK7X9G0h5JL+T39qp834/tnVRbd+VhKkm/nMbYL2mTpA/k6r4m6W5J/5TW8pikt9Z4Gm0UOBxspPUCf0L2Qvdu4D8Cfwgg6Szgh8A/A5cAlwE/ioh/Bv478I10KOVXKgeV9LvAbcBS4GzgA8DeEvP5InA5MDdtbxrw33L1FwHnpPKPA3dLOjfV3Q0cSm1uSDcAIuLqdPdX0py/UWK8SquADWTfq7/Ij58naRJwF7AoIs4Cfg3YGBHPAMuBn6Y5TMl1ux74K+AsoNphp4vSdqel7d4rqeahoUHW3T/XCcA/Aj8ALgD+CPh6xdjXAX8OnAu0p3naGONwsBEVERsiYl1E9ETEC8DfAf8hVb8f2BURd0TE6xFxMCIeKzn0J4DbI2J9ZNoj4heDdZAk4PeBP4mIfRFxkCyEluSaHQU+HxFHI2It8BpwhbITwf8J+FxEdEXE00CZ8x9Vx6syt5nArwJ/FhFHIuJRshfVgfQBb5d0ZkTsjIhNNebx3Yj4SUT0RcTrA7Tp3/YjwD8BH661uBKuBCYDX4iI7oj4F+B7ZIHQ71sR8XhE9ABfJwtuG2McDjaiJF0u6XuSdkl6lezFuP9wyQxgyzCHHk7fZmAisCEd4thPttfSnGuzN71I9esie3FrJjsntyNXl78/kIHGq3QJ8EpEHMqVVQ271OYjZHsJO9MhmV+qMY9ac6227Utq9CnjEmBHRPRVjD0t93hX7v5A3x8bZQ4HG2n3AM8CsyPibLJzCUp1O4CBji/X+njgwfoOZA9wGHhbRExJt3PyJ9MH0Qn0ANNzZTOGuP3B7ATOTYeM+s0cqHFEPBQR7wUuJvv+fqW/aqAuNbZfbdsvpfuHyEK130U1xsp7CZghKf/aMhN4cQhj2BjgcLCRdhbwKvBa+uv2P+fqvgdcJOmP04nisyQtSHUvA7MqXlTy/jfwKUnvUuYySW8ZbCLpr9evAHdKugBA0jRJ76u1iHR1z7eA2yRNTGtZWtHsZeDSWmMNMP4vgDbgzyU1SvoN4LertZV0oaQPpBfzI2SHqvqvPnoZmK7cSf8h6N/2VWSH/P4hlW8EPpTWfRnZuZO8wdb9GFm4/FdJEyRdk9a1ehjzs1HkcLCR9imyk6EHyV6Yj52wTMf830v2YrELeB54T6ruf2HaK+nfKweNiH8gO3G5Ko39HeC8EvP5NNlJz3XpMNcPqXIOYAA3k51c3gX8PfAA2Ytzv9uA+9Ihq+Ecr78eWADsAz4H3D9AuzrgT8n+Kt9Hdg7nD1PdvwCbgF2S9gxh27uAV9KYXweWR8Szqe5OoJssBO5L9Xm3McC6I6Kb7GKBRWR7bl8GlubGtlOE/M9+zMqR9EXgoogY8ru2zU413nMwG4CkX5L0jnQYaz7Z4ZVvj/a8zE4Gv0PabGBnkR1KugTYDdwBfHdUZ2R2kviwkpmZFfiwkpmZFZwWh5WmTp0as2bNGu1pmJmdUjZs2LAnIpqr1Z0W4TBr1iza2tpGexpmZqcUSQN+BI0PK5mZWYHDwczMChwOZmZW4HAwM7MCh4OZmRU4HMzMrMDhYGZmBQ4HMzMrcDiYmVnBafEO6RO16rHtVcuvXzDgf200MzutldpzkLRQ0mZJ7ZJuqVIvSXel+iclzcvVrZS0W9JTFX2+IWljur0gaWMqnyXpcK5uxQmu0czMhqjmnoOkeuBusn/v2AGsl7QmIp7ONVsEzE63BWT/ZL7/fwN/DfgSFf8CMSI+ktvGHcCBXPWWiJg7xLWYmdkIKbPnMB9oj4it6f/DrgYWV7RZDNwfmXXAFEkXA0TEo2T/97YqSQI+TPZPVczMbAwoEw7TgB25xx2pbKhtBnIV8HJEPJ8ra5H0M0mPSLqqWidJyyS1SWrr7OwsuSkzMyujTDioSlnlv48r02Yg13H8XsNOYGZEvBP4JLBK0tmFwSPujYjWiGhtbq76ceRmZjZMZcKhA5iRezwdeGkYbQokNQAfAr7RXxYRRyJib7q/AdgCXF5inmZmNkLKhMN6YLakFkmNwBJgTUWbNcDSdNXSlcCBiNhZYuzfBJ6NiI7+AknN6SQ4ki4lO8m9tcRYZmY2QmperRQRPZJuBh4C6oGVEbFJ0vJUvwJYC1wLtANdwI39/SU9AFwDTJXUAXwuIr6aqpdQPBF9NfB5ST1AL7A8IgY8oW1mZiOv1JvgImItWQDky1bk7gdw0wB9rxtk3I9VKXsQeLDMvMzM7M3hj88wM7MCh4OZmRU4HMzMrMDhYGZmBQ4HMzMrcDiYmVmBw8HMzAocDmZmVuBwMDOzAoeDmZkVOBzMzKzA4WBmZgUOBzMzK3A4mJlZgcPBzMwKHA5mZlbgcDAzswKHg5mZFTgczMyswOFgZmYFpcJB0kJJmyW1S7qlSr0k3ZXqn5Q0L1e3UtJuSU9V9LlN0ouSNqbbtbm6W9NYmyW970QWaGZmQ1czHCTVA3cDi4A5wHWS5lQ0WwTMTrdlwD25uq8BCwcY/s6ImJtua9P25gBLgLelfl9OczAzs5OkzJ7DfKA9IrZGRDewGlhc0WYxcH9k1gFTJF0MEBGPAvuGMKfFwOqIOBIR24D2NAczMztJyoTDNGBH7nFHKhtqm2puToehVko6dyhjSVomqU1SW2dnZ4lNmZlZWWXCQVXKYhhtKt0DvBWYC+wE7hjKWBFxb0S0RkRrc3NzjU2ZmdlQlAmHDmBG7vF04KVhtDlORLwcEb0R0Qd8hTcOHQ15LDMzG1llwmE9MFtSi6RGspPFayrarAGWpquWrgQORMTOwQbtPyeRfBDov5ppDbBEUpOkFrKT3I+XmKeZmY2QhloNIqJH0s3AQ0A9sDIiNklanupXAGuBa8lOHncBN/b3l/QAcA0wVVIH8LmI+Cpwu6S5ZIeMXgD+II23SdI3gaeBHuCmiOgdkdWamVkpiqh1amDsa21tjba2tmH3X/XY9qrl1y+YOewxzczGOkkbIqK1Wp3fIW1mZgUOBzMzK3A4mJlZgcPBzMwKHA5mZlbgcDAzswKHg5mZFTgczMyswOFgZmYFDgczMytwOJiZWYHDwczMChwOZmZW4HAwM7MCh4OZmRU4HMzMrMDhYGZmBQ4HMzMrcDiYmVmBw8HMzApKhYOkhZI2S2qXdEuVekm6K9U/KWlerm6lpN2Snqro89eSnk3tvy1pSiqfJemwpI3ptuIE12hmZkNUMxwk1QN3A4uAOcB1kuZUNFsEzE63ZcA9ubqvAQurDP0w8PaIeAfwHHBrrm5LRMxNt+Ul12JmZiOkzJ7DfKA9IrZGRDewGlhc0WYxcH9k1gFTJF0MEBGPAvsqB42IH0RET3q4Dpg+3EWYmdnIKhMO04AduccdqWyobQbze8D3c49bJP1M0iOSrqrWQdIySW2S2jo7O4ewKTMzq6VMOKhKWQyjTfXBpc8CPcDXU9FOYGZEvBP4JLBK0tmFwSPujYjWiGhtbm4usykzMyupTDh0ADNyj6cDLw2jTYGkG4D3Ax+NiACIiCMRsTfd3wBsAS4vMU8zMxshZcJhPTBbUoukRmAJsKaizRpgabpq6UrgQETsHGxQSQuBTwMfiIiuXHlzOgmOpEvJTnJvLb0iMzM7YQ21GkREj6SbgYeAemBlRGyStDzVrwDWAtcC7UAXcGN/f0kPANcAUyV1AJ+LiK8CXwKagIclAaxLVyZdDXxeUg/QCyyPiMIJbTMze/PUDAeAiFhLFgD5shW5+wHcNEDf6wYov2yA8geBB8vMy8zM3hx+h7SZmRU4HMzMrMDhYGZmBQ4HMzMrcDiYmVmBw8HMzAocDmZmVuBwMDOzAoeDmZkVOBzMzKzA4WBmZgUOBzMzK3A4mJlZgcPBzMwKHA5mZlbgcDAzswKHg5mZFTgczMyswOFgZmYFDgczMysoFQ6SFkraLKld0i1V6iXprlT/pKR5ubqVknZLeqqiz3mSHpb0fPp6bq7u1jTWZknvO5EFmpnZ0NUMB0n1wN3AImAOcJ2kORXNFgGz020ZcE+u7mvAwipD3wL8KCJmAz9Kj0ljLwHelvp9Oc3BzMxOkjJ7DvOB9ojYGhHdwGpgcUWbxcD9kVkHTJF0MUBEPArsqzLuYuC+dP8+4Hdy5asj4khEbAPa0xzMzOwkKRMO04AduccdqWyobSpdGBE7AdLXC4YylqRlktoktXV2dtZchJmZlVcmHFSlLIbRpqxSY0XEvRHRGhGtzc3Nw9yUmZlVUyYcOoAZucfTgZeG0abSy/2HntLX3ScwlpmZjaAy4bAemC2pRVIj2cniNRVt1gBL01VLVwIH+g8ZDWINcEO6fwPw3Vz5EklNklrITnI/XmKeZmY2QhpqNYiIHkk3Aw8B9cDKiNgkaXmqXwGsBa4lO3ncBdzY31/SA8A1wFRJHcDnIuKrwBeAb0r6OLAd+N003iZJ3wSeBnqAmyKid4TWa2ZmJShiuKcGxo7W1tZoa2sbdv9Vj22vWn79gpnDHtPMbKyTtCEiWqvV+R3SZmZW4HAwM7MCh4OZmRU4HMzMrMDhYGZmBQ4HMzMrcDiYmVmBw8HMzAocDmZmVuBwMDOzAoeDmZkVOBzMzKzA4WBmZgUOBzMzK3A4mJlZgcPBzMwKHA5mZlbgcDAzswKHg5mZFTgczMysoFQ4SFooabOkdkm3VKmXpLtS/ZOS5tXqK+kbkjam2wuSNqbyWZIO5+pWjMA6zcxsCBpqNZBUD9wNvBfoANZLWhMRT+eaLQJmp9sC4B5gwWB9I+IjuW3cARzIjbclIuae0MrMzGzYyuw5zAfaI2JrRHQDq4HFFW0WA/dHZh0wRdLFZfpKEvBh4IETXIuZmY2QMuEwDdiRe9yRysq0KdP3KuDliHg+V9Yi6WeSHpF0VbVJSVomqU1SW2dnZ4llmJlZWWXCQVXKomSbMn2v4/i9hp3AzIh4J/BJYJWkswuDRNwbEa0R0drc3Dzg5M3MbOhqnnMg+2t/Ru7xdOClkm0aB+srqQH4EPCu/rKIOAIcSfc3SNoCXA60lZirmZmNgDJ7DuuB2ZJaJDUCS4A1FW3WAEvTVUtXAgciYmeJvr8JPBsRHf0FkprTiWwkXUp2knvrMNdnZmbDUHPPISJ6JN0MPATUAysjYpOk5al+BbAWuBZoB7qAGwfrmxt+CcUT0VcDn5fUA/QCyyNi3wms0czMhkgRlacATj2tra3R1jb8o06rHttetfz6BTOHPaaZ2VgnaUNEtFar8zukzcyswOFgZmYFDgczMytwOJiZWYHDwczMChwOZmZW4HAwM7MCh4OZmRU4HMzMrMDhYGZmBQ4HMzMrcDiYmVmBw8HMzAocDmZmVuBwMDOzAoeDmZkVOBzMzKzA4WBmZgUOBzMzK3A4mJlZQalwkLRQ0mZJ7ZJuqVIvSXel+iclzavVV9Jtkl6UtDHdrs3V3Zrab5b0vhNdpJmZDU1DrQaS6oG7gfcCHcB6SWsi4ulcs0XA7HRbANwDLCjR986I+JuK7c0BlgBvAy4Bfijp8ojoPYF1mpnZEJTZc5gPtEfE1ojoBlYDiyvaLAbuj8w6YIqki0v2rbQYWB0RRyJiG9CexjEzs5OkTDhMA3bkHneksjJtavW9OR2GWinp3CFsD0nLJLVJauvs7CyxDDMzK6tMOKhKWZRsM1jfe4C3AnOBncAdQ9geEXFvRLRGRGtzc3OVLmZmNlw1zzmQ/eU+I/d4OvBSyTaNA/WNiJf7CyV9BfjeELZnZmZvojJ7DuuB2ZJaJDWSnSxeU9FmDbA0XbV0JXAgInYO1jedk+j3QeCp3FhLJDVJaiE7yf34MNdnZmbDUHPPISJ6JN0MPATUAysjYpOk5al+BbAWuJbs5HEXcONgfdPQt0uaS3bI6AXgD1KfTZK+CTwN9AA3+UolM7OTSxGFw/mnnNbW1mhraxt2/1WPba9afv2CmcMe08xsrJO0ISJaq9X5HdJmZlbgcDAzswKHg5mZFTgczMyswOFgZmYFDgczMytwOJiZWYHDwczMChwOZmZW4HAwM7MCh4OZmRU4HMzMrMDhYGZmBQ4HMzMrcDiYmVmBw8HMzAocDmZmVuBwMDOzAoeDmZkVOBwq7H71db6z8UV6evtGeypmZqOmVDhIWihps6R2SbdUqZeku1L9k5Lm1eor6a8lPZvaf1vSlFQ+S9JhSRvTbcUIrLO0zS8f5PFt+3iiY//J3KyZ2ZhSMxwk1QN3A4uAOcB1kuZUNFsEzE63ZcA9Jfo+DLw9It4BPAfcmhtvS0TMTbflw13ccHR19wLwr8/voa8vTuamzczGjDJ7DvOB9ojYGhHdwGpgcUWbxcD9kVkHTJF08WB9I+IHEdGT+q8Dpo/Aek7YoSPZlHYfPMKPn9s9yrMxMxsdZcJhGrAj97gjlZVpU6YvwO8B3889bpH0M0mPSLqq2qQkLZPUJqmts7OzxDLK6eruZerkJs45cwJ/98jWERvXzOxUUiYcVKWs8njLQG1q9pX0WaAH+Hoq2gnMjIh3Ap8EVkk6uzBIxL0R0RoRrc3NzTWWUF5Xdy9nndHAr7/1fB7bto8nduwfsbHNzE4VZcKhA5iRezwdeKlkm0H7SroBeD/w0YgIgIg4EhF70/0NwBbg8jKLGQld3T1MbKznV2edR0OdeGjTrpO1aTOzMaNMOKwHZktqkdQILAHWVLRZAyxNVy1dCRyIiJ2D9ZW0EPg08IGI6OofSFJzOpGNpEvJTnKftOM7Xd29TGxsoGlCPTPPn8i2PYdO1qbNzMaMhloNIqJH0s3AQ0A9sDIiNklanupXAGuBa4F2oAu4cbC+aegvAU3Aw5IA1qUrk64GPi+pB+gFlkfEvpFacI21HttzALh06iSHg5mNSzXDASAi1pIFQL5sRe5+ADeV7ZvKLxug/YPAg2XmNdKO9PTRFzAphUPL1EnHLmmtq6t2+sTM7PTkd0jn9L/HYWJjlpktUydzpKePna++PprTMjM76RwOOV3d2XscJub2HAC2dfrQkpmNLw6HnENH+vccKsJhz2ujNiczs9HgcMg5tufQlB1WuvDsJs6cUM9Wn5Q2s3HG4ZDzxjmHbM9BEi2+YsnMxiGHQ05Xdw8CzphQf6yspdnhYGbjj8Mhp6u7lzMb66nTG5etXjp1Ejv2ddHd4//vYGbjh8Mhp//d0XktUyfRF7B9X9cAvczMTj8Oh5z8u6P7vXHFkg8tmdn44XDIyfYcBgoHX85qZuOHwyGnq7uXSRWHlaZMbOS8SY1s2+PDSmY2fpT6bKXxovKw0qrHtgMwuamBdVv3Hnt8/YKZozI/M7OTxXsOSXdPH0d7o3BYCeCCs5p4af9hX7FkZuOGwyGpfHd03jtnnsuRnj6e7Nh/kmdlZjY6HA5J5buj82adP5ELz27ip1v3kv5hnZnZac3hkFR+XHeeJK689Hx2Hnjd73cws3HB4ZBUflx3pbkzpnDGhDrWbd17MqdlZjYqHA7JYIeVAJoa6pk381yeevFVfrb9lZM5NTOzk25ch0N3Tx9feXQrR3v7OHRsz2Hgq3t/7a1TOWNCHR/88v/jE/et57mXD56sqZqZnVTjOhz+ffsr/NXaZ/jBpl10dffS1FBH/SD/K/q8SY386W9dwad+63LWv/AKv/2//o1Vj233SWozO+2UCgdJCyVtltQu6ZYq9ZJ0V6p/UtK8Wn0lnSfpYUnPp6/n5upuTe03S3rfiS5yIFdeej5L3/0WfrJlL8/tOsikKpexVjpjQj3nTWripvdcxszzJvKZb/+c9975KB/68k/45Dc28p2fvUhPr98PYWZvjq7uHr7/850cOtLzpm5Htf7qlVQPPAe8F+gA1gPXRcTTuTbXAn8EXAssAP5nRCwYrK+k24F9EfGFFBrnRsSnJc0BHgDmA5cAPwQuj4jegebY2toabW1tw/oGHO7u5arb/4U9r3Uz/dwz+cNrLivdty+CR5/r5PFt+0DQUFfHnteOMOO8M7my5Xye3XWQF/YeYu6MKbznigtomlDHMztf5ZWuo7xr5rnMbzmPo719dLxymL4IrrjoLGadP4lXurp5af9hmhrqmXHeRM4+o4EDh4+y91A3k5saOH9SI3US+w8f5dXDRznnzAmcc+YEAA5199Dd08fkMxpoaqinry841N1DAJMaG6ivE319weGjvdTXiaaGOqSsrLu3jwn1b+w99fUFR/v6aKzP2gDH9pKU+1jzvr6srG6QvS6zk2Ggn0+J436Ge/qChjodK6v8WY/Ifh/6+uCMCW/8jnQd7UXAmRPqqasTR3p6Ofh6D40NdUxubECCVw/38OrrRznrjAbOPmMCPX3B7oOvc/D1HqZObuL8SY0cOHyUbXsPceRoH7OmTqR5chNbOg/xRMd+mhrq+JXpUzj7zAk8/PQufry5k7ecP4n3v+NiXtx/mM//49O8uP8wl5xzBn/2/jksfPtFx613KCRtiIjWqnUlwuHdwG0R8b70+Nb0Df4fuTZ/B/w4Ih5IjzcD1wCzBurb3yYidkq6OPW/onJ8SQ+lMX460BxPJBwAbv/nZ1nxyBYuu2AyH/u1lmGP0xfB5l0HeeS5Tva8doRLzjmTKRMn8Iu9XXS+dgTIftDOmFDP/q6jpcetrxO9fW88TxLU6fiy+rrsBzpXRGN9Hd0VezFNDXUcyb3Tu04wof74ssb6OhDHvSO8qaGOAI729hEBDXViQn0dPX3ZO8shK2tsqKPyxzSACAjS/NL9/I9e/8+2EBy7f3x5//3I9Y8qY1Wq9nuj/Ebe+HL8PCr69m8jiIrHVNw5bkNjYvxqY1eOGxUbqPY9Gmj8vog3xoz0mOP79T+3dXrj+Yxc2/6f30iPs346rn3/z/6xfulrX5Wf/bo6ONobx35Pshd+6E4/w5B+roPjfk+aGuro6Yvjfr8qf28kmFB3/O/XQL+X+ceQrb+i6Nj3opoLz25iz2vdx8a54sKz+PhvtLDyJ9t4dtdBPjRvGn/74bnVO9cwWDiU+WylacCO3OMOsr2DWm2m1eh7YUTsBEgBcUFurHVVxjqOpGXAsvTwtRQ2wzUV2LMNePgEBqn0zAiONYKmAntGexInwXhZJ3itp6upwJ5fVBT+AvhB7vGdwJ0fGfY23jJQRZlwqLa/UplxA7Up03c42yMi7gXurTFWKZLaBkrP0814Wet4WSd4raer0V5rmRPSHcCM3OPpwEsl2wzW9+V0OIn0dfcQtmdmZm+iMuGwHpgtqUVSI7AEWFPRZg2wNF21dCVwIB0yGqzvGuCGdP8G4Lu58iWSmiS1ALOBx4e5PjMzG4aah5UiokfSzcBDQD2wMiI2SVqe6lcAa8muVGoHuoAbB+ubhv4C8E1JHwe2A7+b+myS9E3gaaAHuGmwK5VGyIgcnjpFjJe1jpd1gtd6uhrVtda8WsnMzMafcf0OaTMzq87hYGZmBeM6HGp9LMipSNILkn4uaaOktlQ26h9VMhIkrZS0W9JTubIhr03Su9L3qD197MuYe2v3AGu9TdKL6bndmD6ZoL/ulFyrpBmS/q+kZyRtkvRfUvlp97wOstax+bxGxLi8kZ0g3wJcCjQCTwBzRnteI7CuF4CpFWW3A7ek+7cAX0z356R1NwEt6ftRP9prGGRtVwPzgKdOZG1kV7+9m+w9Nd8HFo322kqu9TbgU1XanrJrBS4G5qX7Z5F93M6c0/F5HWStY/J5Hc97DvOB9ojYGhHdwGpg8SjP6c2yGLgv3b8P+J1c+eqIOBIR28iuNpt/8qdXTkQ8CuyrKB7S2tJ7as6OiJ9G9lt2f67PmDHAWgdyyq41InZGxL+n+wfJPlhgGqfh8zrIWgcyqmsdz+Ew0Ed+nOoC+IGkDekjRqDio0qA/EeVnOrfg6GubVq6X1l+qrhZ2Scfr8wdajkt1ippFvBO4DFO8+e1Yq0wBp/X8RwOw/loj1PBr0fEPGARcJOkqwdpe7p+D2BkP9JlrLgHeCswF9gJ3JHKT/m1SpoMPAj8cUS8OljTKmWn+lrH5PM6nsPhtPyYjoh4KX3dDXyb7DDR6fxRJUNdW0e6X1k+5kXEyxHRGxF9wFd44xDgKb1WSRPIXiy/HhHfSsWn5fNaba1j9Xkdz+FQ5mNBTimSJkk6q/8+8FvAU5zeH1UypLWlQxQHJV2ZrvBYmuszpvW/WCYfJHtu4RRea5rXV4FnIuJvc1Wn3fM60FrH7PM62mfwR/NG9pEfz5FdBfDZ0Z7PCKznUrKrG54ANvWvCTgf+BHwfPp6Xq7PZ9P6NzPGru6osr4HyHa7j5L99fTx4awNaCX7BdwCfIn0SQFj6TbAWv8e+DnwJNkLx8Wn+lqB3yA7JPIksDHdrj0dn9dB1jomn1d/fIaZmRWM58NKZmY2AIeDmZkVOBzMzKzA4WBmZgUOBzMzK3A4mJlZgcPBzMwK/j9a2bW4qckxqAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for un-tokenizable pretty printed goals\n",
    "goal_lengths = []\n",
    "for i in trange(len(df)):\n",
    "    try:\n",
    "        tokens = enc.encode(df.cleaned_goal[i])\n",
    "        if enc.decode(tokens) != df.cleaned_goal[i]:\n",
    "            print(\"shit\")\n",
    "            break\n",
    "        goal_lengths.append(len(tokens))\n",
    "    except:\n",
    "        print(f\"Error at {i}, where text is {df.cleaned_goal[i]} with type {type(df.cleaned_goal[i])}\")\n",
    "        raise\n",
    "sns.distplot(goal_lengths)\n",
    "plt.title(\"Goal length distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for un-tokenizable human written tactics\n",
    "tactic_lengths = []\n",
    "for i in trange(len(df)):\n",
    "    try:\n",
    "        tokens = enc.encode(df.human_tactic_code[i])\n",
    "        if enc.decode(tokens) != df.human_tactic_code[i]:\n",
    "            print(\"shit\")\n",
    "            break\n",
    "        tactic_lengths.append(len(tokens))\n",
    "    except:\n",
    "        print(f\"Error at {i}, where text is {df.human_tactic_code[i]} with type {type(df.human_tactic_code[i])}\")\n",
    "        raise\n",
    "sns.distplot(tactic_lengths)\n",
    "plt.title(\"Tactic length distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_indices = []\n",
    "for i in range(len(goal_lengths)):\n",
    "    if goal_lengths[i] + tactic_lengths[i] + 2 <= 1024:\n",
    "        keep_indices.append(i)\n",
    "print(f\"Number of indices to be kept will be {len(keep_indices)} in a total of {len(goal_lengths)}\")\n",
    "\n",
    "print(f\"Length before: {len(df)}\")\n",
    "df = df.iloc[keep_indices, :]\n",
    "df.reset_index()\n",
    "print(f\"Length after: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Put goal and tactic into one place, extend encoder and split train test valid\n",
    "class ProofStep(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "#         self.attn_masks = []\n",
    "#             self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        toks = self.tokenizer.encode(' <|GOAL|>' + self.df.cleaned_goal.iloc[i] + ' <|PROOFSTEP|>' + self.df.human_tactic_code.iloc[i]) \n",
    "        return toks#, self.attn_masks[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '<|GOAL|>', '<|PROOFSTEP|>']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "# Dataset\n",
    "tokenizer = get_encoder()\n",
    "dataset = ProofStep(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157,103 training samples\n",
      "9,818 validation samples\n",
      "29,458 test samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split into train, validation and test set: 80 - 5 - 15\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.05 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} test samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"It was a bright cold day in April.\"\n",
    "quiet=False\n",
    "nsamples=1\n",
    "unconditional=False\n",
    "batch_size=1\n",
    "gradient_acc_steps = 32\n",
    "length=-1\n",
    "temperature=0.7\n",
    "top_k=40\n",
    "\n",
    "epochs=40\n",
    "lr=1e-5\n",
    "warmup_steps=200\n",
    "epsilon = 1e-8\n",
    "\n",
    "output_dir=\"./models\" \n",
    "output_prefix=\"wreckgar\"\n",
    "test_mode=False\n",
    "save_model_on_epoch=True\n",
    "sample_every = 100 #sample output every 100 steps\n",
    "\n",
    "# device=torch.device(\"cpu\")\n",
    "# model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "# For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
