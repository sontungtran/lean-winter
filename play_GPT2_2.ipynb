{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Adoption\n",
    "\n",
    "Code from https://github.com/graykode/gpt-2-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "seed = random.randint(0, 2147483647)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# GPTConfig - HuggingFace style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Config(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size_or_config_json_file=50257,\n",
    "            n_positions=1024,\n",
    "            n_ctx=1024,\n",
    "            n_embd=768,\n",
    "            n_layer=12,\n",
    "            n_head=12,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            initializer_range=0.02,\n",
    "            pdrop=0.1,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size_or_config_json_file\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_positions = n_positions\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.initializer_range = initializer_range\n",
    "        self.pdrop = pdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Wandb managing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtst008\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/tst008/gpt_music_lyrics/runs/49tcwchj\" target=\"_blank\">clear-plasma-27</a></strong> to <a href=\"https://wandb.ai/tst008/gpt_music_lyrics\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: wandb: command not found\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"gpt_music_lyrics\", entity=\"tst008\")\n",
    "!wandb on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Norm: $y = \\frac{x-E[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "- Where x is a mini-batch of input, with dimension `normalized_shape`.\n",
    "- $\\gamma$ and $\\beta$ are $\\textit{learnable}$ affine transform parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = LayerNorm(embedding_dim)\n",
    "layer_norm(embedding).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: $N, C, L$, Output: $N, C, L_o$\n",
    "\n",
    "$out(N, C_j) =  bias(C_j) + \\sum^{C_i - 1}_{k=1}w(C_j, k) * inp(N_i, k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nf, nx):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.nf = nf\n",
    "        self.nx = nx\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = Parameter(w)\n",
    "        self.bias = Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.nx == x.shape[-1], f\"Shape mismatched, expected shape: {list(x.shape[:-1])+[self.nx]}, got shape: {list(x.shape)}\"\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(*size_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Conv1D(10, 50)\n",
    "i = torch.randn(20, 16, 50)\n",
    "m(i).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies multi-head attention \n",
    "\n",
    "- Query: representation of current word used to score against other words\n",
    "- Key: labels for all potentially relevant words in segment\n",
    "- Value: Actual word representations. Relevancy of each word is added up to represent current word\n",
    "\n",
    "Input size, output size??\n",
    "https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False):\n",
    "        super(Attention, self).__init__()\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
    "        assert n_state % config.n_head == 0\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "        self.c_attn = Conv1D(n_state * 3, nx)\n",
    "        self.c_proj = Conv1D(n_state, nx)\n",
    "        self.attn_drop = nn.Dropout(config.pdrop, inplace=False)\n",
    "        self.resid_drop = nn.Dropout(config.pdrop, inplace=False)\n",
    "\n",
    "    def _attn(self, q, k, v):\n",
    "        w = torch.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / math.sqrt(v.size(-1))\n",
    "        nd, ns = w.size(-2), w.size(-1)\n",
    "        try:\n",
    "            b = self.bias[:, :, ns-nd:ns, :ns]\n",
    "            w = w * b - 1e10 * (1 - b)\n",
    "            w = nn.Softmax(dim=-1)(w)\n",
    "        except:\n",
    "            print(b.shape, w.shape)\n",
    "            raise\n",
    "        return w, torch.matmul(w, v)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
    "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
    "\n",
    "    def split_heads(self, x, k=False):\n",
    "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
    "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
    "        if k:\n",
    "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
    "        else:\n",
    "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        # Begin by looking at the shape of all these\n",
    "#         print(f\"x:{x.shape}\")\n",
    "        x = self.c_attn(x)\n",
    "#         print(f\"(after c_attn) x:{x.shape}\")\n",
    "        query, key, value = x.split(self.split_size, dim=2)\n",
    "#         print(f\"query:{query.shape}, key:{key.shape}, value:{value.shape},\")\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "#         print(f\"(after split_heads()) query:{query.shape}, key:{key.shape}, value:{value.shape},\")\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
    "            key = torch.cat((past_key, key), dim=-1)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "#             print(f\"(in if layer_past is not None) query:{query.shape}, key:{key.shape}, value:{value.shape},\")\n",
    "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
    "#         print(f\"present:{present.shape}\")\n",
    "        w, a = self._attn(query, key, value)\n",
    "#         print(f\"(output of _attn) a:{a.shape}\")\n",
    "        a = self.merge_heads(a)\n",
    "#         print(f\"(output of merge_heads) a:{a.shape}\")\n",
    "        a = self.c_proj(a)\n",
    "#         print(f\"(output of c_proj) a:{a.shape}\")\n",
    "        a = self.resid_drop(a)\n",
    "#         print(f\"(output of resid_drop) a:{a.shape}\")\n",
    "        return a, present, w # Present key and values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test Masked before softmax, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones([1, 1, 1024, 1024]) #torch.Size([1, 12, 1760, 1760])\n",
    "b = torch.ones([1, 12,])\n",
    "nd, ns = w.size(-2), w.size(-1)\n",
    "nd, ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies the Gaussian Error Linear Units function:\n",
    "    \n",
    "$y = x * \\Phi(x)$\n",
    "\n",
    "where $\\Phi(x)$ is the Cumulative Distribution Function for Gaussian Distribution\n",
    "\n",
    "... tho I'm not seeing that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0424, -0.1641])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gelu(torch.randn(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D(n_state, n_embd) $\\implies$ gelu $\\implies$ Conv1D(n_embd, n_state)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
    "        super(MLP, self).__init__()\n",
    "        nx = config.n_embd\n",
    "        self.c_fc = Conv1D(n_state, nx)\n",
    "        self.c_proj = Conv1D(nx, n_state)\n",
    "        self.act = gelu\n",
    "        self.dropout = nn.Dropout(config.pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        h2 = self.dropout(h2)\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "config = argparse.Namespace(\n",
    "    n_embd = 15,\n",
    "    pdrop = .1,\n",
    ")\n",
    "i = torch.randn(10, 3, 15)\n",
    "mlp = MLP(30, config)\n",
    "mlp(i).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns: \n",
    "- $x + (LN(x) \\implies ATTN(x)) + MLP(x)$\n",
    "- `present` the cumulative keys and values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_ctx, config, scale=False):\n",
    "        super(Block, self).__init__()\n",
    "        nx = config.n_embd\n",
    "        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.attn = Attention(nx, n_ctx, config, scale)\n",
    "        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(4 * nx, config)\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        a, present, _ = self.attn(self.ln_1(x), layer_past=layer_past)\n",
    "        x = x + a\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "        x = x + m\n",
    "        return x, present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder-only Transformer GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.n_layer = config.n_layer\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_vocab = config.vocab_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        \n",
    "        self.drop = nn.Dropout(config.pdrop, inplace=False)\n",
    "        \n",
    "        block = Block(config.n_ctx, config, scale=True)\n",
    "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, \n",
    "                              eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def set_embeddings_weights(self, model_embeddings_weights):\n",
    "        embed_shape = model_embeddings_weights.shape\n",
    "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
    "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
    "\n",
    "    def forward(self, input_ids, \n",
    "                position_ids=None, \n",
    "                token_type_ids=None, \n",
    "                past=None):\n",
    "        if past is None:\n",
    "            past_length = 0\n",
    "            past = [None] * len(self.h)\n",
    "        else:\n",
    "            past_length = past[0][0].size(-2)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long,\n",
    "                                        device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "        position_ids = position_ids.view(-1, position_ids.size(-1))\n",
    "\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
    "            token_type_embeds = self.wte(token_type_ids)\n",
    "        else:\n",
    "            token_type_embeds = 0\n",
    "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
    "        presents = []\n",
    "        for block, layer_past in zip(self.h, past):\n",
    "            hidden_states, present = block(hidden_states, layer_past)\n",
    "            presents.append(present)\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        output_shape = input_shape + (hidden_states.size(-1),)\n",
    "        return hidden_states.view(*output_shape), presents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2LMHead(nn.Module):\n",
    "    def __init__(self, model_embeddings_weights, config):\n",
    "        super(GPT2LMHead, self).__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.set_embeddings_weights(model_embeddings_weights)\n",
    "\n",
    "    def set_embeddings_weights(self, model_embeddings_weights):\n",
    "        embed_shape = model_embeddings_weights.shape\n",
    "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
    "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        # Truncated Language modeling logits (we remove the last token)\n",
    "        # h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n",
    "        lm_logits = self.decoder(hidden_state)\n",
    "        return lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2LMHeadModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2LMHeadModel, self).__init__()\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n",
    "\n",
    "    def set_tied(self):\n",
    "        \"\"\" Make sure we are sharing the embeddings\n",
    "        \"\"\"\n",
    "        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n",
    "        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        if lm_labels is not None:\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = lm_labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "#             loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            return loss, lm_logits, presents\n",
    "        return lm_logits, presents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "from os.path import expanduser\n",
    "# from functools import lru_cache\n",
    "\n",
    "# @lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder():\n",
    "    home = expanduser(\"~\")\n",
    "    with open(os.path.join(home, 'data/GPT2/encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(home, 'data/GPT2/vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_size_or_config_json_file=50257,\n",
    "n_positions=1024,\n",
    "n_ctx=1024,\n",
    "n_embd=768,\n",
    "n_layer=12,\n",
    "n_head=12,\n",
    "layer_norm_epsilon=1e-5,\n",
    "initializer_range=0.02,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): GPT2LMHead(\n",
       "    (decoder): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "# enc = get_encoder()\n",
    "config = GPT2Config()\n",
    "model = GPT2LMHeadModel(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('gpt2-pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)\n",
    "\n",
    "old_keys = []\n",
    "new_keys = []\n",
    "for key in state_dict.keys():\n",
    "    new_key = None\n",
    "    if key.endswith(\".g\"):\n",
    "        new_key = key[:-2] + \".weight\"\n",
    "    elif key.endswith(\".b\"):\n",
    "        new_key = key[:-2] + \".bias\"\n",
    "    elif key.endswith(\".w\"):\n",
    "        new_key = key[:-2] + \".weight\"\n",
    "    if new_key:\n",
    "        old_keys.append(key)\n",
    "        new_keys.append(new_key)\n",
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    state_dict[new_key] = state_dict.pop(old_key)\n",
    "# model.load_state_dict(state_dict)\n",
    "missing_keys = []\n",
    "unexpected_keys = []\n",
    "error_msgs = []\n",
    "# copy state_dict so _load_from_state_dict can modify it\n",
    "metadata = getattr(state_dict, \"_metadata\", None)\n",
    "state_dict = state_dict.copy()\n",
    "if metadata is not None:\n",
    "    state_dict._metadata = metadata\n",
    "\n",
    "def load(module, prefix=\"\"):\n",
    "    local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "    module._load_from_state_dict(\n",
    "        state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n",
    "    )\n",
    "    for name, child in module._modules.items():\n",
    "        if child is not None:\n",
    "            load(child, prefix + name + \".\")\n",
    "\n",
    "start_model = model\n",
    "if hasattr(model, \"transformer\") and all(not s.startswith('transformer.') for s in state_dict.keys()):\n",
    "    start_model = model.transformer\n",
    "load(start_model, prefix=\"\")\n",
    "\n",
    "model.set_tied()\n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Sampling a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1]\n",
    "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n",
    "\n",
    "def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, device='cuda', sample=True):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "#             print(prev.shape)\n",
    "#             raise\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "    return output\n",
    "\n",
    "# def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, device='cuda', sample=True):\n",
    "#     if start_token is None:\n",
    "#         assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "#         context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "#     else:\n",
    "#         assert context is None, 'Specify exactly one of start_token and context!'\n",
    "#         context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
    "#     prev = context\n",
    "#     output = context\n",
    "#     past = None\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(length):\n",
    "# #             print(prev.shape)\n",
    "# #             raise\n",
    "#             loss, logits, past = model(prev,\n",
    "#                                  lm_labels=prev,\n",
    "#                                  past=past)\n",
    "#             print(loss)\n",
    "#             logits = logits[:, -1, :] / temperature\n",
    "#             logits = top_k_logits(logits, k=top_k)\n",
    "#             log_probs = F.softmax(logits, dim=-1)\n",
    "#             if sample:\n",
    "#                 prev = torch.multinomial(log_probs, num_samples=1)\n",
    "#             else:\n",
    "#                 _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "#             output = torch.cat((output, prev), dim=1)\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Try running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, length=200, nsamples=1, quiet=False, temperature=0.7, text='It was a bright cold day in April.', top_k=40, unconditional=False)\n",
      "It was a bright cold day in April.\n",
      "======================================== SAMPLE 1 ========================================\n",
      " The weather was so bad that the police were forced to take the streets of the village to help evacuate people.\n",
      "\n",
      "\"We got there without any aid from the police. Some people were trapped inside the houses,\" said a policeman.\n",
      "\n",
      "The situation in the village was worse than it had been in the past. A local resident said his wife was taken to the hospital with serious injuries and she had to be evacuated.\n",
      "\n",
      "Police had to clear the streets of the village in search of the perpetrators of the shooting.\n",
      "\n",
      "\"It was quite a shock. There were people on the streets who were crying, and the police were trying to clear them up for us,\" said the local resident.\n",
      "\n",
      "Local residents have asked authorities to make the area safer for everyone.\n",
      "\n",
      "\"People are crying because they are being treated at the hospital. We are asking people to come out and help us,\" said the local resident.\n",
      "\n",
      "The police are working to calm the situation and to establish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Turn into notebook style\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    text=\"It was a bright cold day in April.\",\n",
    "    quiet=False,\n",
    "    nsamples=1,\n",
    "    unconditional=False,\n",
    "    batch_size=1,\n",
    "    length=200,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "if args.quiet is False:\n",
    "    print(args)\n",
    "\n",
    "if args.batch_size == -1:\n",
    "    args.batch_size = 1\n",
    "assert args.nsamples % args.batch_size == 0\n",
    "\n",
    "# Load Model\n",
    "enc = get_encoder()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if args.length == -1:\n",
    "    args.length = config.n_ctx // 2\n",
    "elif args.length > config.n_ctx:\n",
    "    raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
    "\n",
    "print(args.text)\n",
    "context_tokens = enc.encode(args.text)\n",
    "\n",
    "generated = 0\n",
    "for _ in range(args.nsamples // args.batch_size):\n",
    "    out = sample_sequence(\n",
    "        model=model, length=args.length,\n",
    "        context=context_tokens  if not  args.unconditional else None,\n",
    "        start_token=enc.encoder['<|endoftext|>'] if args.unconditional else None,\n",
    "        batch_size=args.batch_size,\n",
    "        temperature=args.temperature, top_k=args.top_k, device=device\n",
    "    )\n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    for i in range(args.batch_size):\n",
    "        generated += 1\n",
    "        text = enc.decode(out[i])\n",
    "        if args.quiet is False:\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            print(text)\n",
    "            \n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use an 80-5-15 train-validation-test split. We split all\n",
    "datapoints deterministically by theorem name, by hashing\n",
    "each name to a float in (0, 1)\n",
    "\n",
    "For fine-tuning a model, we load saved parameters but reinitialize the optimizer. We start each training for a fixed\n",
    "number of tokens (defining the cosine schedule) and record\n",
    "the number of tokens consumed as we reach a minimal validation loss. We use the minimum validation loss snapshot\n",
    "to evaluate each model on our held-out test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Fine Tuning\n",
    "\n",
    "https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's Up</td>\n",
       "      <td>Twenty-five years and my life is still. Trying...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spaceman</td>\n",
       "      <td>Starry night bring me down. Till I realize the...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pleasantly Blue</td>\n",
       "      <td>Every time you wake in the mornin'. And you st...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train</td>\n",
       "      <td>What ya gonna do child. When your thoughts are...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calling All The People</td>\n",
       "      <td>How can you tell, when your wellness is not we...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    SName                                              Lyric  \\\n",
       "0               What's Up  Twenty-five years and my life is still. Trying...   \n",
       "1                Spaceman  Starry night bring me down. Till I realize the...   \n",
       "2         Pleasantly Blue  Every time you wake in the mornin'. And you st...   \n",
       "3                   Train  What ya gonna do child. When your thoughts are...   \n",
       "4  Calling All The People  How can you tell, when your wellness is not we...   \n",
       "\n",
       "          Artist Genre  \n",
       "0  4 Non Blondes  Rock  \n",
       "1  4 Non Blondes  Rock  \n",
       "2  4 Non Blondes  Rock  \n",
       "3  4 Non Blondes  Rock  \n",
       "4  4 Non Blondes  Rock  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data\n",
    "lyrics = pd.read_csv('lyrics-data.csv')\n",
    "lyrics = lyrics[lyrics['Idiom']=='ENGLISH']\n",
    "# Only keep popular artists, with genre Rock/Pop and popularity high enough\n",
    "artists = pd.read_csv('artists-data.csv')\n",
    "artists = artists[(artists['Genre'].isin(['Rock'])) & (artists['Popularity']>5)]\n",
    "\n",
    "df = lyrics.merge(artists[['Artist', 'Genre', 'Link']], \n",
    "                  left_on='ALink', right_on='Link', how='inner')\n",
    "df = df.drop(columns=['ALink','SLink','Idiom','Link'])\n",
    "#Drop the songs with lyrics too long (after more than 1024 tokens, does not work)\n",
    "df = df[df['Lyric'].apply(lambda x: len(x.split(' ')) < 350 and len(x.split(' ')) > 20)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import seaborn as sns\n",
    "# # nltk.download('punkt')\n",
    "# # get rough token count distribution\n",
    "# doc_lengths = []\n",
    "# for row in df['Lyric']:   \n",
    "#     tokens = nltk.word_tokenize(row)\n",
    "#     doc_lengths.append(len(tokens))\n",
    "# doc_lengths = np.array(doc_lengths)\n",
    "# sns.distplot(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongLyrics(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer, truncation=True, gpt2_type=\"gpt2\", max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "#         self.attn_masks = []\n",
    "\n",
    "        for row in df['Lyric']:\n",
    "            toks = self.tokenizer.encode(row + '<|endoftext|>') \n",
    "            self.input_ids.append(torch.tensor(toks[:max_length]))\n",
    "#             self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx]#, self.attn_masks[idx] \n",
    "    \n",
    "#     def print_special_ids(self):\n",
    "#         print(\"The beginning of sequence token {} token has the id {}\".format(\n",
    "#             self.tokenizer.convert_ids_to_tokens(self.tokenizer.bos_token_id), \n",
    "#             self.tokenizer.bos_token_id))\n",
    "#         print(\"The end of sequence token {} has the id {}\".format(\n",
    "#             self.tokenizer.convert_ids_to_tokens(self.tokenizer.eos_token_id), \n",
    "#             self.tokenizer.eos_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AdamW, \n",
    "                          get_linear_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "# Dataset\n",
    "gpt2_type='gpt2'\n",
    "tokenizer = get_encoder()\n",
    "dataset = SongLyrics(df, tokenizer, gpt2_type=gpt2_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,953 training samples\n",
      "  622 validation samples\n",
      "1,867 test samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split into train, validation and test set: 80 - 5 - 15\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.05 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} test samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"It was a bright cold day in April.\"\n",
    "quiet=False\n",
    "nsamples=1\n",
    "unconditional=False\n",
    "batch_size=1\n",
    "gradient_acc_steps = 32\n",
    "length=-1\n",
    "temperature=0.7\n",
    "top_k=40\n",
    "\n",
    "epochs=21\n",
    "lr=1e-5\n",
    "warmup_steps=200\n",
    "epsilon = 1e-8\n",
    "\n",
    "output_dir=\"./models\" \n",
    "output_prefix=\"wreckgar\"\n",
    "test_mode=False\n",
    "save_model_on_epoch=True\n",
    "sample_every = 100 #sample output every 100 steps\n",
    "\n",
    "# device=torch.device(\"cpu\")\n",
    "# model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "# For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output directory ./models\n",
      "Length of current stats file: 0\n",
      "\n",
      "Checkpoint not found, starting from epoch #0, min_val_loss of inf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint model for resuming\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import json\n",
    "\n",
    "def load_checkpoint(model, output_dir, output_prefix):\n",
    "    files = []\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Creating output directory {output_dir}\")\n",
    "    else:    \n",
    "        files = [f for f in os.listdir(output_dir) if isfile(join(output_dir, f))]\n",
    "\n",
    "    # Find training stats\n",
    "    stats_filename = \"stats.json\"\n",
    "    if stats_filename in files:\n",
    "        with open(os.path.join(output_dir, stats_filename), 'r') as f:\n",
    "            training_stats = json.load(f)\n",
    "    else:\n",
    "        training_stats = []\n",
    "    print(f\"Length of current stats file: {len(training_stats)}\\n\")\n",
    "\n",
    "    # Find checkpoints\n",
    "    r = re.compile(f\"{output_prefix}-[0-9]*.pt\")\n",
    "    model_files = list(filter(r.match, files))\n",
    "    epochs_list = [int(x[ len(output_prefix)+1 : -3 ]) for x in model_files]\n",
    "    if epochs_list:\n",
    "        current_epoch = max(epochs_list)\n",
    "        min_val_loss = min([x[\"Valid. Loss\"] for x in training_stats])\n",
    "        config = GPT2Config()\n",
    "        model = GPT2LMHeadModel(config)\n",
    "        model.load_state_dict(\n",
    "                torch.load(os.path.join(output_dir, f\"{output_prefix}-{current_epoch}.pt\"))\n",
    "        )\n",
    "        print(f\"Checkpoint #{current_epoch} found, with min_val_loss of {min_val_loss:.3f}, resuming.\")\n",
    "    else:\n",
    "        current_epoch = 0\n",
    "        min_val_loss = float(\"inf\")\n",
    "        print(f\"Checkpoint not found, starting from epoch #{current_epoch}, min_val_loss of {min_val_loss:.3f}.\")\n",
    "    return model, current_epoch, min_val_loss, training_stats\n",
    "\n",
    "model, current_epoch, min_val_loss, training_stats = load_checkpoint(model, output_dir, output_prefix)\n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-0596964c33e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer = AdamW(model.parameters(),\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.cuda()\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = lr,\n",
    "                  eps = epsilon\n",
    "                )\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = (len(train_dataloader) // gradient_acc_steps + 1) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "print(f\"Number of warmup steps: {warmup_steps}\\nTotal steps: {total_steps}\")\n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "# For sampling\n",
    "def display_attn_and_confidence(model, context_text):\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output[2].detach()\n",
    "        return hook\n",
    "    \n",
    "    context = enc.encode(context_text)\n",
    "    context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # Getting attention outputs\n",
    "    handles = [\n",
    "        model.transformer.h[x].attn.register_forward_hook(get_activation(f'attn_{x}'))\n",
    "            for x in range(12)\n",
    "    ]\n",
    "\n",
    "    # Display next token confidence\n",
    "    prev = context\n",
    "    with torch.no_grad():\n",
    "        logits, past = model(prev)\n",
    "        logits = logits[:, -1, :] / 0.7\n",
    "        logits = top_k_logits(logits, k=10)\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "        values, prev = torch.topk(log_probs, k=10, dim=-1)\n",
    "\n",
    "    prev = prev.tolist()\n",
    "    for i in range(len(prev[0])):\n",
    "        text = enc.decode([prev[0][i]])\n",
    "        print(f\"{text} -> {values[0][i].item()}\")\n",
    "    [handle.remove() for handle in handles]\n",
    "\n",
    "    # Visualize attention maps\n",
    "    idx = 0\n",
    "    input_data = np.arange(context.shape[-1])\n",
    "    attn_maps = [[(torch.sum(activation[f'attn_{x}'], dim=1)/12).detach().cpu().numpy() \n",
    "                         for x in range(12)]]\n",
    "    num_heads = 12\n",
    "    num_layers = 1\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4 if num_heads == 1 else 3\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):        \n",
    "            im = ax[row][column].imshow(attn_maps[row][column][0], origin='upper', vmin=0, vmax=1, cmap='bone')\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(f\"Layer {column+1}\")\n",
    "            context_words = [enc.decode([x]).strip(' ') for x in context.cpu().numpy()[0]]\n",
    "            if column == 0:\n",
    "                ax[row][column].set_yticklabels(context_words)\n",
    "            ax[row][column].set_xticklabels(context_words, rotation=90)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.6)\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cbar_ax = fig.add_axes([0.91, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "# Wandb manage model weights statistics \n",
    "wandb.watch(model)\n",
    "\n",
    "total_t0 = time.time()\n",
    "# training_stats = []\n",
    "# min_val_loss = float('inf')\n",
    "\n",
    "for epoch_i in range(current_epoch+1, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    loss = 0\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        \n",
    "        b_input_ids = batch[0].to(device).unsqueeze(0) # Need to have 2 sizes\n",
    "        \n",
    "#         b_labels = batch[0].to(device)\n",
    "#         b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids,\n",
    "                        lm_labels=b_input_ids,) \n",
    "#                           attention_mask=b_masks,)\\\n",
    "#         print(outputs)\n",
    "#         raise\n",
    "        loss = outputs[0] \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        loss /= gradient_acc_steps \n",
    "        \n",
    "        # Attention table\n",
    "        \n",
    "        # Get sample every x batches.\n",
    "        if (step/gradient_acc_steps) % sample_every == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Step {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "            model.eval()\n",
    "            \n",
    "            # Generate a sample sequence\n",
    "            out = sample_sequence(\n",
    "                model=model, length=200,\n",
    "                context=random.randint(1,30000),\n",
    "                batch_size=1,\n",
    "                temperature=0.95, \n",
    "                top_k=50, device=device\n",
    "            )\n",
    "            out = out[:, len(context_tokens):].tolist()\n",
    "            text = enc.decode(out[0])\n",
    "            print(f\">{text}\")\n",
    "\n",
    "            # Show attention table\n",
    "            context_text = \"You are my fire. The one\"\n",
    "            display_attn_and_confidence(model, context_text)\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        # Update\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step+1) % gradient_acc_steps == 0 or (step + 1 == len(train_dataloader)):            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": epoch_i,\n",
    "                    'batch_loss': batch_loss\n",
    "                }\n",
    "            )\n",
    "            # wandb log attention table\n",
    "#             wandb.join()\n",
    "        \n",
    "    # Report training\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device).unsqueeze(0)\n",
    "#         b_labels = batch[0].to(device)\n",
    "#         b_masks = batch[1].to(device)  \n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs  = model(b_input_ids, \n",
    "                             lm_labels=b_input_ids,) \n",
    "#                            token_type_ids=None, \n",
    "#                              attention_mask = b_masks,          \n",
    "            loss = outputs[0]  \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    # Report validation\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    if min_val_loss > avg_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if save_model_on_epoch:\n",
    "        # Save training stats\n",
    "        with open(os.path.join(output_dir, \"stats.json\"), 'w') as f:\n",
    "            json.dump(training_stats, f)\n",
    "        # Save two files: current best model and current epoch model\n",
    "        if avg_val_loss < min_val_loss:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-best.pt\"),\n",
    "            )\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(output_dir, f\"{output_prefix}-{epoch_i}.pt\"),\n",
    "        )\n",
    "        if epoch_i > 1:\n",
    "            # Remove previous checkpoint\n",
    "            os.remove(os.path.join(output_dir, f\"{output_prefix}-{epoch_i-1}.pt\"))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to generate lyrics for test set, get loss, evaluate on persplexity, bleu and rougev \n",
    "# Want to see model's confidence knowing context as well -> attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check if exists directory, if no, try to create\n",
    "# Try save a dummy model and delete it\n",
    "# These checks are done at the start of notebook\n",
    "# Resume: Check if the directory has a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = GPT2Config()\n",
    "# model = GPT2LMHeadModel(config)\n",
    "model, current_epoch, min_val_loss, training_stats = load_checkpoint(model, output_dir, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nested_tuple(x):\n",
    "    if type(x) == tuple:\n",
    "        print(len(x))\n",
    "        for y in x:\n",
    "            print_nested_tuple(y)\n",
    "    else:\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#               Test\n",
    "# ========================================\n",
    "\n",
    "length = 20\n",
    "temperature=0.7\n",
    "top_k=40\n",
    "model.cuda()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Running Test...\")\n",
    "\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "contexts = []\n",
    "true_lyrics = []\n",
    "generated_lyrics = []\n",
    "# Evaluate data for one epoch\n",
    "for i, batch in tqdm(enumerate(test_dataloader)):\n",
    "#     print(batch.shape)\n",
    "    \n",
    "#     if i % 1 == 0:\n",
    "#         print(f\"{i}/{len(test_dataloader)}\")\n",
    "    \n",
    "    b_input_ids = batch.to(device).view(batch_size, -1)\n",
    "#     print(b_input_ids.shape)\n",
    "#         b_labels = batch[0].to(device)\n",
    "#         b_masks = batch[1].to(device)  \n",
    "\n",
    "    context = b_input_ids[:,:-length].view(batch_size, -1)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():   \n",
    "        # Generate text\n",
    "        for i in range(length):\n",
    "            try:\n",
    "                logits, past = model(prev, \n",
    "                                 past=past)\n",
    "            except:\n",
    "                print(b_input_ids.shape)\n",
    "                raise\n",
    "#             print(logits.shape)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "#             if sample:\n",
    "            prev = torch.multinomial(log_probs, num_samples=1)\n",
    "#             else:\n",
    "#                 _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "          \n",
    "        # Find loss\n",
    "        loss, _, _  = model(b_input_ids, \n",
    "                         lm_labels=b_input_ids,)\n",
    "    \n",
    "    batch_loss = loss.item()\n",
    "    total_eval_loss += batch_loss\n",
    "    \n",
    "    output = output[:, len(context):].tolist()\n",
    "    text = enc.decode(output[0])\n",
    "#     print(f\"{context}\\n->{text}\")\n",
    "    \n",
    "    contexts.append(enc.decode(list(context[0].cpu().numpy())))\n",
    "    true_lyrics.append(enc.decode(list(batch.view(-1).cpu().numpy())[-length:]))\n",
    "    generated_lyrics.append(text)\n",
    "    \n",
    "#     print(f\"Outputs have {len(outputs)} items:\\noutputs[0]={outputs[0]}\\noutputs[1]={outputs[1].shape}\\noutputs[2]={[x.shape for x in outputs[2]]}\")\n",
    "#     if i >= 3:\n",
    "#         break\n",
    "    \n",
    "# Report validation\n",
    "avg_test_loss = total_eval_loss / len(test_dataloader)\n",
    "test_time = format_time(time.time() - t0)    \n",
    "print(\"  Test Loss: {0:.2f}\".format(avg_test_loss)) #2.25\n",
    "print(\"  Test took: {:}\".format(test_time))\n",
    "len(contexts), len(true_lyrics), len(generated_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts[7]+\"|||\"+true_lyrics[7], contexts[7]+\"|||\"+generated_lyrics[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(true_lyrics)):\n",
    "    to_remove = generated_lyrics[i].split('.')[-1]\n",
    "    final.append(generated_lyrics[i].replace(to_remove,''))\n",
    "\n",
    "# test_set['Generated_lyrics'] = final\n",
    "# test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(true_lyrics)):\n",
    "    reference = [true_lyrics[i]]\n",
    "    candidate = final[i]\n",
    "    scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge score\n",
    "from rouge import Rouge\n",
    "rouge=Rouge()\n",
    "\n",
    "rouge.get_scores(final, true_lyrics, avg=True, ignore_empty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "# # Create the DataLoaders for our datasets.\n",
    "# # We'll take training samples in random order. \n",
    "# train_dataloader = DataLoader(\n",
    "#             train_dataset,  # The training samples.\n",
    "#             sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "#             batch_size = batch_size # Trains with this batch size.\n",
    "#         )\n",
    "# # For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "# validation_dataloader = DataLoader(\n",
    "#             val_dataset, # The validation samples.\n",
    "#             sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "#         )\n",
    "# test_dataloader = DataLoader(\n",
    "#             test_dataset, # The validation samples.\n",
    "#             sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry of the matrix is a row in attention table (for wandb logging)\n",
    "        # Log wandb\n",
    "#         columns=[\"s_ind\", \"t_ind\", \"s_word\", \"t_word\", \"attn\"]\n",
    "#         attn_table = wandb.Table(columns=columns)\n",
    "#         temp = context_text.split(\" \")\n",
    "#         for s_ind in range(seq_len):\n",
    "#                 attn_table.add_data(s_ind, t_ind, temp[s_ind], temp[t_ind], attn_maps[row][column][0][s_ind][t_ind])\n",
    "#         wandb.log({\"attn_table\": attn_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
