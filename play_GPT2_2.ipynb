{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Adoption\n",
    "\n",
    "Code from https://github.com/graykode/gpt-2-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "seed = random.randint(0, 2147483647)\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# GPTConfig - HuggingFace style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Config(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size_or_config_json_file=50257,\n",
    "            n_positions=1024,\n",
    "            n_ctx=1024,\n",
    "            n_embd=768,\n",
    "            n_layer=12,\n",
    "            n_head=12,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            initializer_range=0.02,\n",
    "            pdrop=0.1,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size_or_config_json_file\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_positions = n_positions\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.initializer_range = initializer_range\n",
    "        self.pdrop = pdrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Wandb managing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtst008\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/tst008/gpt_music_lyrics/runs/1ny21ol3\" target=\"_blank\">celestial-star-28</a></strong> to <a href=\"https://wandb.ai/tst008/gpt_music_lyrics\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B online, running your script from this directory will now sync to the cloud.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"gpt_music_lyrics\", entity=\"tst008\")\n",
    "!wandb on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Norm: $y = \\frac{x-E[x]}{\\sqrt{Var[x] + \\epsilon}} * \\gamma + \\beta$\n",
    "\n",
    "- Where x is a mini-batch of input, with dimension `normalized_shape`.\n",
    "- $\\gamma$ and $\\beta$ are $\\textit{learnable}$ affine transform parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = LayerNorm(embedding_dim)\n",
    "layer_norm(embedding).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: $N, C, L$, Output: $N, C, L_o$\n",
    "\n",
    "$out(N, C_j) =  bias(C_j) + \\sum^{C_i - 1}_{k=1}w(C_j, k) * inp(N_i, k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nf, nx):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.nf = nf\n",
    "        self.nx = nx\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = Parameter(w)\n",
    "        self.bias = Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.nx == x.shape[-1], f\"Shape mismatched, expected shape: {list(x.shape[:-1])+[self.nx]}, got shape: {list(x.shape)}\"\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(*size_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Conv1D(10, 50)\n",
    "i = torch.randn(20, 16, 50)\n",
    "m(i).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies multi-head attention \n",
    "\n",
    "- Query: representation of current word used to score against other words\n",
    "- Key: labels for all potentially relevant words in segment\n",
    "- Value: Actual word representations. Relevancy of each word is added up to represent current word\n",
    "\n",
    "Input size, output size??\n",
    "https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False):\n",
    "        super(Attention, self).__init__()\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
    "        assert n_state % config.n_head == 0\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "        self.c_attn = Conv1D(n_state * 3, nx)\n",
    "        self.c_proj = Conv1D(n_state, nx)\n",
    "        self.attn_drop = nn.Dropout(config.pdrop, inplace=False)\n",
    "        self.resid_drop = nn.Dropout(config.pdrop, inplace=False)\n",
    "\n",
    "    def _attn(self, q, k, v):\n",
    "        w = torch.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / math.sqrt(v.size(-1))\n",
    "        nd, ns = w.size(-2), w.size(-1)\n",
    "        try:\n",
    "            b = self.bias[:, :, ns-nd:ns, :ns]\n",
    "            w = w * b - 1e10 * (1 - b)\n",
    "            w = nn.Softmax(dim=-1)(w)\n",
    "        except:\n",
    "            print(b.shape, w.shape)\n",
    "            raise\n",
    "        return w, torch.matmul(w, v)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
    "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
    "\n",
    "    def split_heads(self, x, k=False):\n",
    "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
    "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
    "        if k:\n",
    "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
    "        else:\n",
    "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        # Begin by looking at the shape of all these\n",
    "#         print(f\"x:{x.shape}\")\n",
    "        x = self.c_attn(x)\n",
    "#         print(f\"(after c_attn) x:{x.shape}\")\n",
    "        query, key, value = x.split(self.split_size, dim=2)\n",
    "#         print(f\"query:{query.shape}, key:{key.shape}, value:{value.shape},\")\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "#         print(f\"(after split_heads()) query:{query.shape}, key:{key.shape}, value:{value.shape},\")\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
    "            key = torch.cat((past_key, key), dim=-1)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "#             print(f\"(in if layer_past is not None) query:{query.shape}, key:{key.shape}, value:{value.shape},\")\n",
    "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
    "#         print(f\"present:{present.shape}\")\n",
    "        w, a = self._attn(query, key, value)\n",
    "#         print(f\"(output of _attn) a:{a.shape}\")\n",
    "        a = self.merge_heads(a)\n",
    "#         print(f\"(output of merge_heads) a:{a.shape}\")\n",
    "        a = self.c_proj(a)\n",
    "#         print(f\"(output of c_proj) a:{a.shape}\")\n",
    "        a = self.resid_drop(a)\n",
    "#         print(f\"(output of resid_drop) a:{a.shape}\")\n",
    "        return a, present, w # Present key and values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test Masked before softmax, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones([1, 1, 1024, 1024]) #torch.Size([1, 12, 1760, 1760])\n",
    "b = torch.ones([1, 12,])\n",
    "nd, ns = w.size(-2), w.size(-1)\n",
    "nd, ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies the Gaussian Error Linear Units function:\n",
    "    \n",
    "$y = x * \\Phi(x)$\n",
    "\n",
    "where $\\Phi(x)$ is the Cumulative Distribution Function for Gaussian Distribution\n",
    "\n",
    "... tho I'm not seeing that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.6200])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gelu(torch.randn(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D(n_state, n_embd) $\\implies$ gelu $\\implies$ Conv1D(n_embd, n_state)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
    "        super(MLP, self).__init__()\n",
    "        nx = config.n_embd\n",
    "        self.c_fc = Conv1D(n_state, nx)\n",
    "        self.c_proj = Conv1D(nx, n_state)\n",
    "        self.act = gelu\n",
    "        self.dropout = nn.Dropout(config.pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        h2 = self.dropout(h2)\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "config = argparse.Namespace(\n",
    "    n_embd = 15,\n",
    "    pdrop = .1,\n",
    ")\n",
    "i = torch.randn(10, 3, 15)\n",
    "mlp = MLP(30, config)\n",
    "mlp(i).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns: \n",
    "- $x + (LN(x) \\implies ATTN(x)) + MLP(x)$\n",
    "- `present` the cumulative keys and values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_ctx, config, scale=False):\n",
    "        super(Block, self).__init__()\n",
    "        nx = config.n_embd\n",
    "        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.attn = Attention(nx, n_ctx, config, scale)\n",
    "        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(4 * nx, config)\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        a, present, _ = self.attn(self.ln_1(x), layer_past=layer_past)\n",
    "        x = x + a\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "        x = x + m\n",
    "        return x, present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder-only Transformer GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.n_layer = config.n_layer\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_vocab = config.vocab_size\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        \n",
    "        self.drop = nn.Dropout(config.pdrop, inplace=False)\n",
    "        \n",
    "        block = Block(config.n_ctx, config, scale=True)\n",
    "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, \n",
    "                              eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def set_embeddings_weights(self, model_embeddings_weights):\n",
    "        embed_shape = model_embeddings_weights.shape\n",
    "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
    "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
    "\n",
    "    def forward(self, input_ids, \n",
    "                position_ids=None, \n",
    "                token_type_ids=None, \n",
    "                past=None):\n",
    "        if past is None:\n",
    "            past_length = 0\n",
    "            past = [None] * len(self.h)\n",
    "        else:\n",
    "            past_length = past[0][0].size(-2)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long,\n",
    "                                        device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "        position_ids = position_ids.view(-1, position_ids.size(-1))\n",
    "\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
    "            token_type_embeds = self.wte(token_type_ids)\n",
    "        else:\n",
    "            token_type_embeds = 0\n",
    "        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n",
    "        presents = []\n",
    "        for block, layer_past in zip(self.h, past):\n",
    "            hidden_states, present = block(hidden_states, layer_past)\n",
    "            presents.append(present)\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        output_shape = input_shape + (hidden_states.size(-1),)\n",
    "        return hidden_states.view(*output_shape), presents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2LMHead(nn.Module):\n",
    "    def __init__(self, model_embeddings_weights, config):\n",
    "        super(GPT2LMHead, self).__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.set_embeddings_weights(model_embeddings_weights)\n",
    "\n",
    "    def set_embeddings_weights(self, model_embeddings_weights):\n",
    "        embed_shape = model_embeddings_weights.shape\n",
    "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
    "        self.decoder.weight = model_embeddings_weights  # Tied weights\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        # Truncated Language modeling logits (we remove the last token)\n",
    "        # h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)\n",
    "        lm_logits = self.decoder(hidden_state)\n",
    "        return lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2LMHeadModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2LMHeadModel, self).__init__()\n",
    "        self.transformer = GPT2Model(config)\n",
    "        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n",
    "\n",
    "    def set_tied(self):\n",
    "        \"\"\" Make sure we are sharing the embeddings\n",
    "        \"\"\"\n",
    "        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n",
    "        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        if lm_labels is not None:\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = lm_labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "#             loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "#             loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            return loss, lm_logits, presents\n",
    "        return lm_logits, presents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "from os.path import expanduser\n",
    "# from functools import lru_cache\n",
    "\n",
    "# @lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "        self.special_tokens = []\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.re_pattern = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        self.pat = re.compile(self.re_pattern)\n",
    "        \n",
    "    def add_special_token(self, s):\n",
    "        if s in self.special_tokens:\n",
    "            print(f\"Token {s} already added\")\n",
    "        self.special_tokens.append(s)\n",
    "        if s in self.encoder: \n",
    "            # Special token already known, but regex pattern cannot capture fully yet\n",
    "            pass\n",
    "        else:\n",
    "            self.encoder[s] = len(self.encoder)+1\n",
    "            self.decoder[len(self.encoder)+1] = s\n",
    "        s = s.replace(\"|\", r\"\\|\")\n",
    "        self.re_pattern = \" \" + s + \"|\" + self.re_pattern\n",
    "        self.pat = re.compile(self.re_pattern)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        # Also don't split on special tokens\n",
    "        if token in self.special_tokens:\n",
    "            return token\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder():\n",
    "    home = expanduser(\"~\")\n",
    "    with open(os.path.join(home, 'data/GPT2/encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(home, 'data/GPT2/vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    enc = Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )\n",
    "    enc.add_special_token(\"<|endoftext|>\")\n",
    "    enc.add_special_token(\"<|GOAL|>\")\n",
    "    enc.add_special_token(\"<|PROOFSTEP|>\")\n",
    "    print(enc.special_tokens)\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_size_or_config_json_file=50257,\n",
    "n_positions=1024,\n",
    "n_ctx=1024,\n",
    "n_embd=768,\n",
    "n_layer=12,\n",
    "n_head=12,\n",
    "layer_norm_epsilon=1e-5,\n",
    "initializer_range=0.02,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "          (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): GPT2LMHead(\n",
       "    (decoder): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "# enc = get_encoder()\n",
    "config = GPT2Config()\n",
    "model = GPT2LMHeadModel(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('gpt2-pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)\n",
    "\n",
    "old_keys = []\n",
    "new_keys = []\n",
    "for key in state_dict.keys():\n",
    "    new_key = None\n",
    "    if key.endswith(\".g\"):\n",
    "        new_key = key[:-2] + \".weight\"\n",
    "    elif key.endswith(\".b\"):\n",
    "        new_key = key[:-2] + \".bias\"\n",
    "    elif key.endswith(\".w\"):\n",
    "        new_key = key[:-2] + \".weight\"\n",
    "    if new_key:\n",
    "        old_keys.append(key)\n",
    "        new_keys.append(new_key)\n",
    "for old_key, new_key in zip(old_keys, new_keys):\n",
    "    state_dict[new_key] = state_dict.pop(old_key)\n",
    "# model.load_state_dict(state_dict)\n",
    "missing_keys = []\n",
    "unexpected_keys = []\n",
    "error_msgs = []\n",
    "# copy state_dict so _load_from_state_dict can modify it\n",
    "metadata = getattr(state_dict, \"_metadata\", None)\n",
    "state_dict = state_dict.copy()\n",
    "if metadata is not None:\n",
    "    state_dict._metadata = metadata\n",
    "\n",
    "def load(module, prefix=\"\"):\n",
    "    local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "    module._load_from_state_dict(\n",
    "        state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n",
    "    )\n",
    "    for name, child in module._modules.items():\n",
    "        if child is not None:\n",
    "            load(child, prefix + name + \".\")\n",
    "\n",
    "start_model = model\n",
    "if hasattr(model, \"transformer\") and all(not s.startswith('transformer.') for s in state_dict.keys()):\n",
    "    start_model = model.transformer\n",
    "load(start_model, prefix=\"\")\n",
    "\n",
    "model.set_tied()\n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Sampling a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1]\n",
    "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n",
    "\n",
    "def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, device='cuda', sample=True):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "#             print(prev.shape)\n",
    "#             raise\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "    return output\n",
    "\n",
    "# def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, device='cuda', sample=True):\n",
    "#     if start_token is None:\n",
    "#         assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "#         context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "#     else:\n",
    "#         assert context is None, 'Specify exactly one of start_token and context!'\n",
    "#         context = torch.full((batch_size, 1), start_token, device=device, dtype=torch.long)\n",
    "#     prev = context\n",
    "#     output = context\n",
    "#     past = None\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(length):\n",
    "# #             print(prev.shape)\n",
    "# #             raise\n",
    "#             loss, logits, past = model(prev,\n",
    "#                                  lm_labels=prev,\n",
    "#                                  past=past)\n",
    "#             print(loss)\n",
    "#             logits = logits[:, -1, :] / temperature\n",
    "#             logits = top_k_logits(logits, k=top_k)\n",
    "#             log_probs = F.softmax(logits, dim=-1)\n",
    "#             if sample:\n",
    "#                 prev = torch.multinomial(log_probs, num_samples=1)\n",
    "#             else:\n",
    "#                 _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "#             output = torch.cat((output, prev), dim=1)\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Try running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, length=200, nsamples=1, quiet=False, temperature=0.7, text='It was a bright cold day in April.', top_k=40, unconditional=False)\n",
      "['<|endoftext|>', '<|GOAL|>', '<|PROOFSTEP|>']\n",
      "It was a bright cold day in April.\n",
      "======================================== SAMPLE 1 ========================================\n",
      " A little over a year before he was killed in a gunfight with a SWAT team, James G. was working as an accountant at a local bank. He and his partner had been working as an accounting and tax analyst for a time. But when he was shot, he said, he was convinced it was an accident. He called the FBI.\n",
      "\n",
      "\"The FBI told me they would investigate it, but I didn't want to let them know,\" he said. \"They only told me there were a couple of dozen people with guns on the street, and it was a very frightening thing that happened. I would never have believed they would do something like this.\"\n",
      "\n",
      "Advertisement Continue reading the main story\n",
      "\n",
      "James G. was killed Jan. 8, 2004. No one in the street has ever been charged with the crime. The federal Bureau of Alcohol, Tobacco, Firearms and Explosives said it did not have a motive to kill James G.\n",
      "\n",
      "In a statement, the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Turn into notebook style\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    text=\"It was a bright cold day in April.\",\n",
    "    quiet=False,\n",
    "    nsamples=1,\n",
    "    unconditional=False,\n",
    "    batch_size=1,\n",
    "    length=200,\n",
    "    temperature=0.7,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "if args.quiet is False:\n",
    "    print(args)\n",
    "\n",
    "if args.batch_size == -1:\n",
    "    args.batch_size = 1\n",
    "assert args.nsamples % args.batch_size == 0\n",
    "\n",
    "# Load Model\n",
    "enc = get_encoder()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if args.length == -1:\n",
    "    args.length = config.n_ctx // 2\n",
    "elif args.length > config.n_ctx:\n",
    "    raise ValueError(\"Can't get samples longer than window size: %s\" % config.n_ctx)\n",
    "\n",
    "print(args.text)\n",
    "context_tokens = enc.encode(args.text)\n",
    "\n",
    "generated = 0\n",
    "for _ in range(args.nsamples // args.batch_size):\n",
    "    out = sample_sequence(\n",
    "        model=model, length=args.length,\n",
    "        context=context_tokens  if not  args.unconditional else None,\n",
    "        start_token=enc.encoder['<|endoftext|>'] if args.unconditional else None,\n",
    "        batch_size=args.batch_size,\n",
    "        temperature=args.temperature, top_k=args.top_k, device=device\n",
    "    )\n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    for i in range(args.batch_size):\n",
    "        generated += 1\n",
    "        text = enc.decode(out[i])\n",
    "        if args.quiet is False:\n",
    "            print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            print(text)\n",
    "            \n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use an 80-5-15 train-validation-test split. We split all\n",
    "datapoints deterministically by theorem name, by hashing\n",
    "each name to a float in (0, 1)\n",
    "\n",
    "For fine-tuning a model, we load saved parameters but reinitialize the optimizer. We start each training for a fixed\n",
    "number of tokens (defining the cosine schedule) and record\n",
    "the number of tokens consumed as we reach a minimal validation loss. We use the minimum validation loss snapshot\n",
    "to evaluate each model on our held-out test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "# Fine Tuning\n",
    "\n",
    "https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's Up</td>\n",
       "      <td>Twenty-five years and my life is still. Trying...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spaceman</td>\n",
       "      <td>Starry night bring me down. Till I realize the...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pleasantly Blue</td>\n",
       "      <td>Every time you wake in the mornin'. And you st...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train</td>\n",
       "      <td>What ya gonna do child. When your thoughts are...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calling All The People</td>\n",
       "      <td>How can you tell, when your wellness is not we...</td>\n",
       "      <td>4 Non Blondes</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    SName                                              Lyric  \\\n",
       "0               What's Up  Twenty-five years and my life is still. Trying...   \n",
       "1                Spaceman  Starry night bring me down. Till I realize the...   \n",
       "2         Pleasantly Blue  Every time you wake in the mornin'. And you st...   \n",
       "3                   Train  What ya gonna do child. When your thoughts are...   \n",
       "4  Calling All The People  How can you tell, when your wellness is not we...   \n",
       "\n",
       "          Artist Genre  \n",
       "0  4 Non Blondes  Rock  \n",
       "1  4 Non Blondes  Rock  \n",
       "2  4 Non Blondes  Rock  \n",
       "3  4 Non Blondes  Rock  \n",
       "4  4 Non Blondes  Rock  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data\n",
    "lyrics = pd.read_csv('lyrics-data.csv')\n",
    "lyrics = lyrics[lyrics['Idiom']=='ENGLISH']\n",
    "# Only keep popular artists, with genre Rock/Pop and popularity high enough\n",
    "artists = pd.read_csv('artists-data.csv')\n",
    "artists = artists[(artists['Genre'].isin(['Rock'])) & (artists['Popularity']>5)]\n",
    "\n",
    "df = lyrics.merge(artists[['Artist', 'Genre', 'Link']], \n",
    "                  left_on='ALink', right_on='Link', how='inner')\n",
    "df = df.drop(columns=['ALink','SLink','Idiom','Link'])\n",
    "#Drop the songs with lyrics too long (after more than 1024 tokens, does not work)\n",
    "df = df[df['Lyric'].apply(lambda x: len(x.split(' ')) < 350 and len(x.split(' ')) > 20)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import seaborn as sns\n",
    "# # nltk.download('punkt')\n",
    "# # get rough token count distribution\n",
    "# doc_lengths = []\n",
    "# for row in df['Lyric']:   \n",
    "#     tokens = nltk.word_tokenize(row)\n",
    "#     doc_lengths.append(len(tokens))\n",
    "# doc_lengths = np.array(doc_lengths)\n",
    "# sns.distplot(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongLyrics(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer, truncation=True, gpt2_type=\"gpt2\", max_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "#         self.attn_masks = []\n",
    "\n",
    "        for row in df['Lyric']:\n",
    "            toks = self.tokenizer.encode(row + '<|endoftext|>') \n",
    "            self.input_ids.append(torch.tensor(toks[:max_length]))\n",
    "#             self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx]#, self.attn_masks[idx] \n",
    "    \n",
    "#     def print_special_ids(self):\n",
    "#         print(\"The beginning of sequence token {} token has the id {}\".format(\n",
    "#             self.tokenizer.convert_ids_to_tokens(self.tokenizer.bos_token_id), \n",
    "#             self.tokenizer.bos_token_id))\n",
    "#         print(\"The end of sequence token {} has the id {}\".format(\n",
    "#             self.tokenizer.convert_ids_to_tokens(self.tokenizer.eos_token_id), \n",
    "#             self.tokenizer.eos_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AdamW, \n",
    "                          get_linear_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '<|GOAL|>', '<|PROOFSTEP|>']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "# Dataset\n",
    "gpt2_type='gpt2'\n",
    "tokenizer = get_encoder()\n",
    "dataset = SongLyrics(df, tokenizer, gpt2_type=gpt2_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,953 training samples\n",
      "  622 validation samples\n",
      "1,867 test samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split into train, validation and test set: 80 - 5 - 15\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.05 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} test samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"It was a bright cold day in April.\"\n",
    "quiet=False\n",
    "nsamples=1\n",
    "unconditional=False\n",
    "batch_size=1\n",
    "gradient_acc_steps = 32\n",
    "length=-1\n",
    "temperature=0.7\n",
    "top_k=40\n",
    "\n",
    "epochs=21\n",
    "lr=1e-5\n",
    "warmup_steps=200\n",
    "epsilon = 1e-8\n",
    "\n",
    "output_dir=\"./models\" \n",
    "output_prefix=\"wreckgar\"\n",
    "test_mode=False\n",
    "save_model_on_epoch=True\n",
    "sample_every = 100 #sample output every 100 steps\n",
    "\n",
    "# device=torch.device(\"cpu\")\n",
    "# model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "# Create the DataLoaders for our datasets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "# For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of current stats file: 0\n",
      "\n",
      "Checkpoint not found, starting from epoch #0, min_val_loss of inf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint model for resuming\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import json\n",
    "\n",
    "def load_checkpoint(model, config, output_dir, output_prefix):\n",
    "    files = []\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Creating output directory {output_dir}\")\n",
    "    else:    \n",
    "        files = [f for f in os.listdir(output_dir) if isfile(join(output_dir, f))]\n",
    "\n",
    "    # Find training stats\n",
    "    stats_filename = \"stats.json\"\n",
    "    if stats_filename in files:\n",
    "        with open(os.path.join(output_dir, stats_filename), 'r') as f:\n",
    "            training_stats = json.load(f)\n",
    "    else:\n",
    "        training_stats = []\n",
    "    print(f\"Length of current stats file: {len(training_stats)}\\n\")\n",
    "\n",
    "    # Find checkpoints\n",
    "    r = re.compile(f\"{output_prefix}-[0-9]*.pt\")\n",
    "    model_files = list(filter(r.match, files))\n",
    "    epochs_list = [int(x[ len(output_prefix)+1 : -3 ]) for x in model_files]\n",
    "    if epochs_list:\n",
    "        current_epoch = max(epochs_list)\n",
    "        min_val_loss = min([x[\"Valid. Loss\"] for x in training_stats])\n",
    "        model = GPT2LMHeadModel(config)\n",
    "        model.load_state_dict(\n",
    "                torch.load(os.path.join(output_dir, f\"{output_prefix}-{current_epoch}.pt\"))\n",
    "        )\n",
    "        print(f\"Checkpoint #{current_epoch} found, with min_val_loss of {min_val_loss:.3f}, resuming.\")\n",
    "    else:\n",
    "        current_epoch = 0\n",
    "        min_val_loss = float(\"inf\")\n",
    "        print(f\"Checkpoint not found, starting from epoch #{current_epoch}, min_val_loss of {min_val_loss:.3f}.\")\n",
    "    return model, current_epoch, min_val_loss, training_stats\n",
    "\n",
    "config = GPT2Config()\n",
    "config.vocab_size += 2 # 2 new tokens\n",
    "model, current_epoch, min_val_loss, training_stats = load_checkpoint(model, config, output_dir, output_prefix)\n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of warmup steps: 200\n",
      "Total steps: 6552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.cuda()\n",
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = lr,\n",
    "                  eps = epsilon\n",
    "                )\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = (len(train_dataloader) // gradient_acc_steps + 1) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = warmup_steps, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "print(f\"Number of warmup steps: {warmup_steps}\\nTotal steps: {total_steps}\")\n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "# For sampling\n",
    "def display_attn_and_confidence(model, context_text):\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output[2].detach()\n",
    "        return hook\n",
    "    \n",
    "    context = enc.encode(context_text)\n",
    "    context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # Getting attention outputs\n",
    "    handles = [\n",
    "        model.transformer.h[x].attn.register_forward_hook(get_activation(f'attn_{x}'))\n",
    "            for x in range(12)\n",
    "    ]\n",
    "\n",
    "    # Display next token confidence\n",
    "    prev = context\n",
    "    with torch.no_grad():\n",
    "        logits, past = model(prev)\n",
    "        logits = logits[:, -1, :] / 0.7\n",
    "        logits = top_k_logits(logits, k=10)\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "        values, prev = torch.topk(log_probs, k=10, dim=-1)\n",
    "\n",
    "    prev = prev.tolist()\n",
    "    for i in range(len(prev[0])):\n",
    "        text = enc.decode([prev[0][i]])\n",
    "        print(f\"{text} -> {values[0][i].item()}\")\n",
    "    [handle.remove() for handle in handles]\n",
    "\n",
    "    # Visualize attention maps\n",
    "    idx = 0\n",
    "    input_data = np.arange(context.shape[-1])\n",
    "    attn_maps = [[(torch.sum(activation[f'attn_{x}'], dim=1)/12).detach().cpu().numpy() \n",
    "                         for x in range(12)]]\n",
    "    num_heads = 12\n",
    "    num_layers = 1\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4 if num_heads == 1 else 3\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):        \n",
    "            im = ax[row][column].imshow(attn_maps[row][column][0], origin='upper', vmin=0, vmax=1, cmap='bone')\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(f\"Layer {column+1}\")\n",
    "            context_words = [enc.decode([x]).strip(' ') for x in context.cpu().numpy()[0]]\n",
    "            if column == 0:\n",
    "                ax[row][column].set_yticklabels(context_words)\n",
    "            ax[row][column].set_xticklabels(context_words, rotation=90)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.6)\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cbar_ax = fig.add_axes([0.91, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 17.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 21 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3199it [02:22, 23.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 3,200  of  9,953. Loss: 2.2067947387695312.   Elapsed: 0:02:22.\n",
      ">text.getAttribute(\"text\"): text = text[-1].text() if (text.getAttribute(\"text\"): font_family = (text.getAttribute(\"font_family\"))) if (text.getAttribute(\"text\"): font_size = (text.getAttribute(\"font_size\"))) if (text.getAttribute(\"text\"): font_style = FontStyle(text.getAttribute(\"font_style\"))) if (text.getAttribute(\"text\"): font_height = text.getAttribute(\"font_height\"))) text.setAttribute(\"text\",FontStyle(text.getAttribute(\"font_style\"))) return text.append(_text) text = text.getAttribute(\"text\",FontStyle(text.getAttribute(\"font_style\"))) return \"\\\"\" return text.append(_text) return Text() endfunction\n",
      "\n",
      "RAW Paste Data\n",
      "\n",
      "def __\n",
      " who -> 0.7840346097946167\n",
      " that -> 0.0744779109954834\n",
      " I -> 0.06253274530172348\n",
      " you -> 0.03470554202795029\n",
      " with -> 0.021662075072526932\n",
      " thing -> 0.010480610653758049\n",
      " in -> 0.0038482840172946453\n",
      " and -> 0.003033461980521679\n",
      " to -> 0.0026582968421280384\n",
      " of -> 0.002566484035924077\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2592x216 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6398it [04:53, 22.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 6,400  of  9,953. Loss: 3.8945603370666504.   Elapsed: 0:04:54.\n",
      ">Sidney and her boyfriend, Josh, had taken a couple of flights to England this week for a little-known concert in Manchester. She's done some shopping in Manchester, but couldn't seem to find his favourite hotel. There's a pub in Manchester called The Oakenfold, and she found a way back to the real place.\n",
      "\n",
      "As it turns out, she's spent the past couple of weeks searching for a hotel she can get herself into. As she goes in for a last minute night, it's not quite the hotel. It's the one she has to hide in a corner.\n",
      "\n",
      "In the early hours, they got into a fight. They've been on the go for so long it's almost like you're going into a movie. She says, \"It's been so long that I don't even want to sleep in a hotel at all.\"\n",
      "\n",
      "But she's determined to find the sweet spot if she\n",
      " who -> 0.7136647701263428\n",
      " that -> 0.11128803342580795\n",
      " I -> 0.08779929578304291\n",
      " you -> 0.046990204602479935\n",
      " with -> 0.01980013959109783\n",
      " thing -> 0.004910365678369999\n",
      " in -> 0.004781709518283606\n",
      " to -> 0.0038158323150128126\n",
      " and -> 0.0035371005069464445\n",
      " of -> 0.003412487916648388\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2592x216 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9600it [07:27, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 9,600  of  9,953. Loss: 2.4706356525421143.   Elapsed: 0:07:28.\n",
      "> round the world. I knew it, for now I think it's broken. I saw it, but it's broken. And now I still doubt it. It's broken. I see it, but it's broken. And now I still doubt it. It's broken. I see it, but it's broken. And now I still doubt it. It's broken. [8:40:54 PM] Daphne: I know, but it's broken. I saw it. But it's broken. [8:40:54 PM] Daphne: I know it, but it's broken. I saw it, but it's broken. [8:40:54 PM] Daphne: I know it, but it's broken. I saw it, but it's broken. [8:40:54 PM] Daphne: I know it, but it's broken. I saw it, but it\n",
      " who -> 0.6134127974510193\n",
      " that -> 0.15923276543617249\n",
      " I -> 0.1300155520439148\n",
      " you -> 0.05735042691230774\n",
      " with -> 0.020901139825582504\n",
      " thing -> 0.004289435222744942\n",
      " to -> 0.004204801749438047\n",
      " in -> 0.0038783762138336897\n",
      " of -> 0.003534805728122592\n",
      " and -> 0.0031799618154764175\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2592x216 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9953it [07:47, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 2.76\n",
      "  Training epoch took: 0:07:48\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.58\n",
      "  Validation took: 0:00:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 17.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 21 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1018it [00:48, 20.03it/s]"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "# Wandb manage model weights statistics \n",
    "wandb.watch(model)\n",
    "\n",
    "total_t0 = time.time()\n",
    "# training_stats = []\n",
    "# min_val_loss = float('inf')\n",
    "\n",
    "for epoch_i in range(current_epoch+1, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    loss = 0\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        \n",
    "        b_input_ids = batch[0].to(device).unsqueeze(0) # Need to have 2 sizes\n",
    "        \n",
    "#         b_labels = batch[0].to(device)\n",
    "#         b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids,\n",
    "                        lm_labels=b_input_ids,) \n",
    "#                           attention_mask=b_masks,)\\\n",
    "#         print(outputs)\n",
    "#         raise\n",
    "        loss = outputs[0] \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "        loss /= gradient_acc_steps \n",
    "        \n",
    "        # Attention table\n",
    "        \n",
    "        # Get sample every x batches.\n",
    "        if (step/gradient_acc_steps) % sample_every == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Step {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "            model.eval()\n",
    "            \n",
    "            # Generate a sample sequence\n",
    "            out = sample_sequence(\n",
    "                model=model, length=200,\n",
    "                context=random.randint(1,30000),\n",
    "                batch_size=1,\n",
    "                temperature=0.95, \n",
    "                top_k=50, device=device\n",
    "            )\n",
    "            out = out[:, len(context_tokens):].tolist()\n",
    "            text = enc.decode(out[0])\n",
    "            print(f\">{text}\")\n",
    "\n",
    "            # Show attention table\n",
    "            context_text = \"You are my fire. The one\"\n",
    "            display_attn_and_confidence(model, context_text)\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        # Update\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step+1) % gradient_acc_steps == 0 or (step + 1 == len(train_dataloader)):            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": epoch_i,\n",
    "                    'batch_loss': batch_loss\n",
    "                }\n",
    "            )\n",
    "            # wandb log attention table\n",
    "#             wandb.join()\n",
    "        \n",
    "    # Report training\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device).unsqueeze(0)\n",
    "#         b_labels = batch[0].to(device)\n",
    "#         b_masks = batch[1].to(device)  \n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs  = model(b_input_ids, \n",
    "                             lm_labels=b_input_ids,) \n",
    "#                            token_type_ids=None, \n",
    "#                              attention_mask = b_masks,          \n",
    "            loss = outputs[0]  \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    # Report validation\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    if min_val_loss > avg_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if save_model_on_epoch:\n",
    "        # Save training stats\n",
    "        with open(os.path.join(output_dir, \"stats.json\"), 'w') as f:\n",
    "            json.dump(training_stats, f)\n",
    "        # Save two files: current best model and current epoch model\n",
    "        if avg_val_loss < min_val_loss:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-best.pt\"),\n",
    "            )\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(output_dir, f\"{output_prefix}-{epoch_i}.pt\"),\n",
    "        )\n",
    "        if epoch_i > 1:\n",
    "            # Remove previous checkpoint\n",
    "            os.remove(os.path.join(output_dir, f\"{output_prefix}-{epoch_i-1}.pt\"))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "type(model) == GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to generate lyrics for test set, get loss, evaluate on persplexity, bleu and rougev \n",
    "# Want to see model's confidence knowing context as well -> attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check if exists directory, if no, try to create\n",
    "# Try save a dummy model and delete it\n",
    "# These checks are done at the start of notebook\n",
    "# Resume: Check if the directory has a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = GPT2Config()\n",
    "# model = GPT2LMHeadModel(config)\n",
    "model, current_epoch, min_val_loss, training_stats = load_checkpoint(model, output_dir, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nested_tuple(x):\n",
    "    if type(x) == tuple:\n",
    "        print(len(x))\n",
    "        for y in x:\n",
    "            print_nested_tuple(y)\n",
    "    else:\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#               Test\n",
    "# ========================================\n",
    "\n",
    "length = 20\n",
    "temperature=0.7\n",
    "top_k=40\n",
    "model.cuda()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Running Test...\")\n",
    "\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "contexts = []\n",
    "true_lyrics = []\n",
    "generated_lyrics = []\n",
    "# Evaluate data for one epoch\n",
    "for i, batch in tqdm(enumerate(test_dataloader)):\n",
    "#     print(batch.shape)\n",
    "    \n",
    "#     if i % 1 == 0:\n",
    "#         print(f\"{i}/{len(test_dataloader)}\")\n",
    "    \n",
    "    b_input_ids = batch.to(device).view(batch_size, -1)\n",
    "#     print(b_input_ids.shape)\n",
    "#         b_labels = batch[0].to(device)\n",
    "#         b_masks = batch[1].to(device)  \n",
    "\n",
    "    context = b_input_ids[:,:-length].view(batch_size, -1)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():   \n",
    "        # Generate text\n",
    "        for i in range(length):\n",
    "            try:\n",
    "                logits, past = model(prev, \n",
    "                                 past=past)\n",
    "            except:\n",
    "                print(b_input_ids.shape)\n",
    "                raise\n",
    "#             print(logits.shape)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "#             if sample:\n",
    "            prev = torch.multinomial(log_probs, num_samples=1)\n",
    "#             else:\n",
    "#                 _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "          \n",
    "        # Find loss\n",
    "        loss, _, _  = model(b_input_ids, \n",
    "                         lm_labels=b_input_ids,)\n",
    "    \n",
    "    batch_loss = loss.item()\n",
    "    total_eval_loss += batch_loss\n",
    "    \n",
    "    output = output[:, len(context):].tolist()\n",
    "    text = enc.decode(output[0])\n",
    "#     print(f\"{context}\\n->{text}\")\n",
    "    \n",
    "    contexts.append(enc.decode(list(context[0].cpu().numpy())))\n",
    "    true_lyrics.append(enc.decode(list(batch.view(-1).cpu().numpy())[-length:]))\n",
    "    generated_lyrics.append(text)\n",
    "    \n",
    "#     print(f\"Outputs have {len(outputs)} items:\\noutputs[0]={outputs[0]}\\noutputs[1]={outputs[1].shape}\\noutputs[2]={[x.shape for x in outputs[2]]}\")\n",
    "#     if i >= 3:\n",
    "#         break\n",
    "    \n",
    "# Report validation\n",
    "avg_test_loss = total_eval_loss / len(test_dataloader)\n",
    "test_time = format_time(time.time() - t0)    \n",
    "print(\"  Test Loss: {0:.2f}\".format(avg_test_loss)) #2.25\n",
    "print(\"  Test took: {:}\".format(test_time))\n",
    "len(contexts), len(true_lyrics), len(generated_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts[7]+\"|||\"+true_lyrics[7], contexts[7]+\"|||\"+generated_lyrics[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(true_lyrics)):\n",
    "    to_remove = generated_lyrics[i].split('.')[-1]\n",
    "    final.append(generated_lyrics[i].replace(to_remove,''))\n",
    "\n",
    "# test_set['Generated_lyrics'] = final\n",
    "# test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(true_lyrics)):\n",
    "    reference = [true_lyrics[i]]\n",
    "    candidate = final[i]\n",
    "    scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge score\n",
    "from rouge import Rouge\n",
    "rouge=Rouge()\n",
    "\n",
    "rouge.get_scores(final, true_lyrics, avg=True, ignore_empty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "# # Create the DataLoaders for our datasets.\n",
    "# # We'll take training samples in random order. \n",
    "# train_dataloader = DataLoader(\n",
    "#             train_dataset,  # The training samples.\n",
    "#             sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "#             batch_size = batch_size # Trains with this batch size.\n",
    "#         )\n",
    "# # For validation and test the order doesn't matter, so we'll just read them sequentially.\n",
    "# validation_dataloader = DataLoader(\n",
    "#             val_dataset, # The validation samples.\n",
    "#             sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "#         )\n",
    "# test_dataloader = DataLoader(\n",
    "#             test_dataset, # The validation samples.\n",
    "#             sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "#             batch_size = batch_size # Evaluate with this batch size.\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry of the matrix is a row in attention table (for wandb logging)\n",
    "        # Log wandb\n",
    "#         columns=[\"s_ind\", \"t_ind\", \"s_word\", \"t_word\", \"attn\"]\n",
    "#         attn_table = wandb.Table(columns=columns)\n",
    "#         temp = context_text.split(\" \")\n",
    "#         for s_ind in range(seq_len):\n",
    "#                 attn_table.add_data(s_ind, t_ind, temp[s_ind], temp[t_ind], attn_maps[row][column][0][s_ind][t_ind])\n",
    "#         wandb.log({\"attn_table\": attn_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
